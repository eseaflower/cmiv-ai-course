{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning and Medical Imaging - Radiology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "We want to classify an image (patch) from an MR scan into one of 2 categories, {non-tumor,tumor}. Given such a classifier we could run it over all the image patches in an image to get a segmentation mask.\n",
    "\n",
    "To do this we frame the problem as trying to estimate the conditional probabillity of the class given the image pixels, $f(x)=p(y|x)$, where $f(x)$ is the function we are trying to learn. For $f(x)$ to be a valid probabillity distribution we only require that $f_{i}(x)\\ge 0$ and that $\\sum_{i=0}^n f_{i} = 1$. A common trick in machine learning to convert any function into a probabillity distribution is to define $g_{i}(x) = \\frac{e^{x_{i}}}{\\sum_{j=0}^n e^{x_{j}}}$, $g$ is called the softmax function. By applying the softmax to a vector valued function we end up with a valid probabillity distribution.\n",
    "We will use this to learn an \"unconstrained\" real valued function and apply a softmax to convert the output into a probabillity distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the below lines to download and unpack the data when running in Colab\n",
    "# !wget -O data.tar https://github.com/fordanic/cmiv-ai-course/raw/master/data.tar\n",
    "# !tar -xvf data.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Activation, BatchNormalization, Dropout, Dense, Flatten\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.datasets import mnist\n",
    "from keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_device():\n",
    "    device_string = '/cpu:0'\n",
    "    gpu=None\n",
    "    if gpu is not None:\n",
    "        device_string='/device:GPU:{0}'.format(gpu)\n",
    "    return tf.device(device_string)\n",
    "\n",
    "def _init_keras():\n",
    "    # Setup the session to dynamically allocate memory\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    K.set_session(session)\n",
    "\n",
    "# Init the default session to be used by Keras\n",
    "_init_keras()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data():\n",
    "    rootdir = os.path.abspath(os.getcwd())\n",
    "    datadir = os.path.join(os.path.join(rootdir, \"data\"), \"MR\")\n",
    "    pos_pattern = os.path.join(os.path.join(datadir,\"positive_images\"), \"*.jpg\")\n",
    "    neg_pattern = os.path.join(os.path.join(datadir,\"negative_images\"), \"*.jpg\")\n",
    "    pos_filenames = list(glob.glob(pos_pattern))\n",
    "    neg_filenames = list(glob.glob(neg_pattern))\n",
    "\n",
    "    pos_images = [matplotlib.image.imread(fname) for fname in pos_filenames]\n",
    "    neg_images = [matplotlib.image.imread(fname) for fname in neg_filenames]\n",
    "    X = np.vstack([np.array(pos_images, dtype=np.float32), np.array(neg_images, dtype=np.float32)])\n",
    "    y = np.array([1]*len(pos_images) + [0]*len(neg_images), dtype=np.int32)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will download data and cache it.\n",
    "(X_train, y_train), (X_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of the training and test sets, aswell as the dimension of each array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the image patches by plotting some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_patches(X, y, y_true=None, to_plot=None):    \n",
    "    to_plot = to_plot or len(X)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    for i in range(to_plot):\n",
    "        plt.subplot(1, to_plot, i+1)\n",
    "        plt.imshow(X[i].reshape((32, 32)), interpolation='nearest', cmap='gray')\n",
    "        plt.text(0, 0, y[i], color='black', \n",
    "                 bbox=dict(facecolor='white', alpha=1))\n",
    "        if y_true is not None:\n",
    "            plt.text(0, 32, y_true[i], color='black', \n",
    "                     bbox=dict(facecolor='white', alpha=1))\n",
    "            \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_patches(X_train, y_train, to_plot=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras models\n",
    "\n",
    "We will be using Keras (https://keras.io/) with the TensorFlow (https://www.tensorflow.org/) backend to explore some of the concepts in deep learning.\n",
    "\n",
    "The idea of Keras (and TensorFlow and Theano which are the two backend supported by Keras) is to represent your machine learning problem as a computation/data flow graph. The graph is defined in some language (tyically Python) and then the graph is compiled into efficient operations (typically written in C++/CUDA) and executed on either CPU or GPU.\n",
    "To compose the graph, Keras supplies a number of building blocks that can be put together to express most graphs of interest. All the imports at the top from `keras.layers` specify the building blocks that we will use in the exercise.\n",
    "Let's start by defining a very simple model for classifying these image patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_model():\n",
    "    # Images are 1 channel and 32x32\n",
    "    input = Input(shape=(32, 32, 1))\n",
    "    # Flatten the image into a 784-dimensional vector\n",
    "    x = Flatten()(input)\n",
    "    # Multiply the image-vector with a (32x32)x50 matrix and add a bias\n",
    "    # Then apply an elementwise ReLU (max(0, x))\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "    # The output from the previous layer is a 50-dimensional vector\n",
    "    # for each image example. Use this vector to categorize the image\n",
    "    # as one of the classes 0-1\n",
    "    x = Dense(2)(x)\n",
    "    # Return a model that represents the computation graph that maps\n",
    "    # input to the 10 class scores.\n",
    "    return Model(input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returned from this function will take the role of $f(x)$ in the problem description. To construct a learning problem we also need a cost function that can measure how well a certain $f(x)$ approximates the true $p(y|x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "We will be using the categorical cross-entropy cost function (https://en.wikipedia.org/wiki/Cross_entropy) when training the model.\n",
    "\n",
    "The model defined above represents a computation that takes as input an image (or a bacth of images) and generates \"scores\" for each of the classes {0,1}, the scores represent the unnormalized probabillites for each class (the logits). To get a proper probabillity distribution we should apply a softmax to the values. We will skip this step since we are only interested in the relative size of the score for different classes (i.e. which class has the highest score). Naively applying a softmax when using categorical cross-entropy as cost function could also introduce numerical instabillity (taking the gradient of the log of a softmax). To circumvent this, but still use the categorical cross-entropy we can \"cheat\" a bit and directly use TensorFlows built in cost function for this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy_logits(y_true, y_pred):\n",
    "    return K.categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "The last part in defining the learning problem is to define how we should update the parameters of the model given the training data. We know that we should compute the gradient of the loss function with respect to the parameters of the model and move the parameters in the opposite direction. Computing the gradient by hand can be really tricky, fortunately Keras, TensorFlow and Theano all come with automatic differentiation. Using the chain-rule of calculus, the gradient with respect to the model parameters can be computed automatically. There are also different schemes for using the gradient to compute how to update the parameters, all this comes pre-packaged in Keras Optimizers. We will start with the workhorse of optimization, Stochastic Gradient Descent, SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_device():\n",
    "    # Build model\n",
    "    model = build_mlp_model()\n",
    "    # Use Stochastic Gradient Descent as optimizer\n",
    "    optimizer = SGD(lr=0.003)\n",
    "    # Compile the model, giving it our custom loss-function\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=crossentropy_logits, \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras dataformat\n",
    "\n",
    "In Keras an image is represented as a 3D tensor, width x height x channels. The image patches on disk represents the image only as width x height. The labels in the data are represented as plain numbers ({0,1}). When using cross-entropy in Keras the target distribution should be a \"1-hot\" encoding of the label (this corresponds to a target distribution where all probabillity is placed on the correct label).\n",
    "To fix this we define a function that converts the data from the plain data format to a format the can be used within Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensors(X, y):\n",
    "    # Convert X into a 4D tensor and y into a 2D tensor with 1-hot encoding\n",
    "    return X[:, :, :, np.newaxis], to_categorical(y, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = to_tensors(X_train, y_train)\n",
    "X_test, y_test = to_tensors(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to track the progress of the learning we create a callback object that will log some metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCallback(Callback):            \n",
    "    def on_epoch_end(self, epoch, logs=None):                                        \n",
    "        print(\"{}: L: {:.7} A: {:.7} VL: {:.7} VA: {:.7}\".format(epoch,                                                                            \n",
    "                                                                 logs['loss'], \n",
    "                                                                 logs['acc'], \n",
    "                                                                 logs['val_loss'], \n",
    "                                                                 logs['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "To train the model and visualize the result we will call fit() on the model. We can start by training on only a fraction of the data. This is helpfull to spot any errors or parameters that might need tuning before investing more time training on the full dataset. (Set verbose=0 otherwise the browser might hang due to a lot of output.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the following lines produces an error saying ValueError,\n",
    "# try uncommenting the following lines and run this cell again\n",
    "\n",
    "#def crossentropy_logits(y_true, y_pred):\n",
    "#    return K.categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "\n",
    "# If this made a difference, you have to go through all the code below and swap places on \n",
    "# the variables y_true and y_pred when calling K.categorical_crossentropy\n",
    "\n",
    "# Use the first 5000 examples to train on\n",
    "n_train = 5000\n",
    "with get_device():\n",
    "    # Fit the model and save the logs\n",
    "    logs = model.fit(X_train[:n_train], y_train[:n_train], \n",
    "                     validation_split=0.3, \n",
    "                     epochs=50, \n",
    "                     verbose=0, \n",
    "                     callbacks=[LogCallback()])\n",
    "# Plot the accuracy on the training data and the validation data\n",
    "plt.plot(logs.history['acc'], c='r')\n",
    "plt.plot(logs.history['val_acc'], c='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Until now we have only evaluated the model on the training data (the validation data is part of the development). To evaluate how well the model can be expected to perform we should always test it on the test set. The test set SHOULD NOT be used to tune any parameters. The test set is used as the last verification of the model, we can also use it to explore what type of errors the model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "y_proba_test = model.predict(X_test)\n",
    "# Compute the class with the highest score for each example\n",
    "y_pred_test = np.argmax(y_proba_test, axis=-1)\n",
    "# Compute the error vector\n",
    "errors = y_pred_test != np.argmax(y_test, axis=-1)\n",
    "# Compute the accuracy    \n",
    "print(\"Accuracy: {}\".format(1.0-np.mean(errors)))\n",
    "\n",
    "# Plot the first examples.\n",
    "to_evaluate = 5\n",
    "X_eval = X_test[:to_evaluate]    \n",
    "y_eval = y_pred_test[:to_evaluate]\n",
    "plot_patches(X_eval, y_eval)\n",
    "\n",
    "# Plot the first error examples\n",
    "X_eval = X_test[np.where(errors)][:to_evaluate]\n",
    "y_eval = y_pred_test[np.where(errors)][:to_evaluate]\n",
    "plot_patches(X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different values of the learning rate (tip: Start by looking at a log-scale 0.1, 0.01 ...)\n",
    "# Try an optimizer with adaptive learning rate, Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train with all data (tip: Setting n_train=-1 will slice all the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the size of the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dropout regularization layer (x = Dropout(0.5)(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full code (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Lambda, Activation, BatchNormalization, Dropout, Dense, Flatten\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.datasets import mnist\n",
    "from keras.callbacks import Callback\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_device():\n",
    "    device_string = '/cpu:0'\n",
    "    gpu=None\n",
    "    if gpu is not None:\n",
    "        device_string='/device:GPU:{0}'.format(gpu)\n",
    "    return tf.device(device_string)\n",
    "\n",
    "def _init_keras():\n",
    "    # Setup the session to dynamically allocate memory\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    K.set_session(session)\n",
    "\n",
    "# Init the default session to be used by Keras\n",
    "_init_keras()\n",
    "\n",
    "def load_data():\n",
    "    rootdir = os.path.abspath(os.getcwd())\n",
    "    datadir = os.path.join(os.path.join(rootdir, \"data\"), \"MR\")\n",
    "    pos_pattern = os.path.join(os.path.join(datadir,\"positive_images\"), \"*.jpg\")\n",
    "    neg_pattern = os.path.join(os.path.join(datadir,\"negative_images\"), \"*.jpg\")\n",
    "    pos_filenames = list(glob.glob(pos_pattern))\n",
    "    neg_filenames = list(glob.glob(neg_pattern))\n",
    "\n",
    "    pos_images = [matplotlib.image.imread(fname) for fname in pos_filenames]\n",
    "    neg_images = [matplotlib.image.imread(fname) for fname in neg_filenames]\n",
    "    X = np.vstack([np.array(pos_images, dtype=np.float32), np.array(neg_images, dtype=np.float32)])\n",
    "    y = np.array([1]*len(pos_images) + [0]*len(neg_images), dtype=np.int32)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "\n",
    "def crossentropy_logits(y_true, y_pred):\n",
    "    return K.categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "\n",
    "def build_mlp_model():\n",
    "    # Images are 1 channel and 32x32\n",
    "    input = Input(shape=(32, 32, 1))\n",
    "    x = Flatten()(input)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(512, activation='relu')(x)    \n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(512, activation='relu')(x)    \n",
    "    x = Dense(2)(x)\n",
    "    return Model(input, x)\n",
    "\n",
    "def to_tensors(X, y):\n",
    "    return X[:, :, :, np.newaxis], to_categorical(y, num_classes=2)\n",
    "\n",
    "def plot_patches(X, y, y_true=None, to_plot=None):    \n",
    "    to_plot = to_plot or len(X)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    for i in range(to_plot):\n",
    "        plt.subplot(1, to_plot, i+1)\n",
    "        plt.imshow(X[i].reshape((32, 32)), interpolation='nearest', cmap='gray')\n",
    "        plt.text(0, 0, y[i], color='black', \n",
    "                 bbox=dict(facecolor='white', alpha=1))\n",
    "        if y_true is not None:\n",
    "            plt.text(0, 32, y_true[i], color='black', \n",
    "                     bbox=dict(facecolor='white', alpha=1))\n",
    "            \n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "class LogCallback(Callback):            \n",
    "    def on_epoch_end(self, epoch, logs=None):                                        \n",
    "        print(\"{}: L: {:.7} A: {:.7} VL: {:.7} VA: {:.7}\".format(epoch,                                                                            \n",
    "                                                                 logs['loss'], \n",
    "                                                                 logs['acc'], \n",
    "                                                                 logs['val_loss'], \n",
    "                                                                 logs['val_acc']))    \n",
    "    \n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "plot_patches(X_train, y_train, to_plot=10)\n",
    "\n",
    "X_train, y_train = to_tensors(X_train, y_train)\n",
    "X_test, y_test = to_tensors(X_test, y_test)\n",
    "\n",
    "with get_device():\n",
    "    model = build_mlp_model()\n",
    "\n",
    "    optimizer = Adam()\n",
    "    model.compile(optimizer=optimizer, loss = crossentropy_logits, metrics=['accuracy'])\n",
    "\n",
    "    n_train = -1\n",
    "    logs = model.fit(X_train[:n_train], y_train[:n_train], \n",
    "                     validation_split=0.3, \n",
    "                     epochs=5,\n",
    "                     verbose=0, callbacks=[LogCallback()])\n",
    "plt.plot(logs.history['acc'])\n",
    "plt.plot(logs.history['val_acc'])\n",
    "plt.show()\n",
    "\n",
    "# Predict on test data\n",
    "y_proba_test = model.predict(X_test)\n",
    "y_pred_test = np.argmax(y_proba_test, axis=-1)\n",
    "y_true = np.argmax(y_test, axis=-1)\n",
    "errors = y_pred_test != y_true\n",
    "\n",
    "# Compute the accuracy    \n",
    "print(\"Accuracy: {}\".format(1.0-np.mean(errors)))\n",
    "\n",
    "# Plot the first examples.\n",
    "to_evaluate = 15\n",
    "X_eval = X_test[:to_evaluate]    \n",
    "y_eval = y_pred_test[:to_evaluate]\n",
    "plot_patches(X_eval, y_eval, y_true=y_true[:to_evaluate])\n",
    "\n",
    "# Plot the first error examples\n",
    "X_eval = X_test[np.where(errors)][:to_evaluate]\n",
    "y_eval = y_pred_test[np.where(errors)][:to_evaluate]\n",
    "plot_patches(X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "The model above was a standard feed-forward network where all nodes in a previous layer are connected to all nodes in the next. This strategy leads to an explosion of the number of parameters in the model when the size of the input image increases. When dealing with images we often use a hypothesis of stationarity in the image, pixels in a neighbourhood are correlated simliarilly across the entire image, the absolute coordinate (x,y) of a pixel does not influence its distribution. Using this hypothesis we can share weights across the image, thus reducing the total nbumber of parameters that need to be learned. This is the idea behind Convolutional Neural Networks.\n",
    "\n",
    "In CNNs the parameters of the model is convolved across the image to produce feature maps. The feature maps produced in one layer can be used as input to another convolution layer in the same way as layers are stacked in a feed-forward network. To introduce some translation invariance into the model the output feature maps are pooled at certain stages. This effectively increases the receptive field of later convolutions, allowing them to \"see\" a larger part of the input.\n",
    "\n",
    "Lets perform the same task as before, tumor classification, but using convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv_model():\n",
    "    # Images are 1 channel and 32x32\n",
    "    input = Input(shape=(32, 32, 1))\n",
    "    \n",
    "    # Compute 32 feature maps by convolving with 3x3 kernels\n",
    "    x = Conv2D(32, (3, 3), padding=\"same\", activation='relu')(input)\n",
    "    # Output is now 32x32x32\n",
    "    # Pool it to produce 32x16x16 feature maps\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(2)(x)\n",
    "    return Model(input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_device():\n",
    "    # Build model\n",
    "    model = build_conv_model()\n",
    "    # Use Adam\n",
    "    optimizer = Adam()\n",
    "    # Compile the model, giving it our custom loss-function\n",
    "    model.compile(optimizer=optimizer, loss = crossentropy_logits, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first 5000 examples to train on\n",
    "n_train = 5000\n",
    "with get_device():\n",
    "    # Fit the model and save the logs\n",
    "    logs = model.fit(X_train[:n_train], y_train[:n_train], \n",
    "                     validation_split=0.3, \n",
    "                     epochs=5,\n",
    "                     verbose=0,\n",
    "                     callbacks=[LogCallback()])\n",
    "# Plot the accuracy on the training data and the validation data\n",
    "plt.plot(logs.history['acc'], c='r')\n",
    "plt.plot(logs.history['val_acc'], c='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER: Try the model with a few samples (5000) then use the full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different sizes of the kernels (use odd sizes, 3, 5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to add a second Conv-Pool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the Flatten-layer add a normal Dense-layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full code (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Lambda, Activation, BatchNormalization, Dropout, Dense, Flatten\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.datasets import mnist\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_device():\n",
    "    device_string = '/cpu:0'\n",
    "    gpu=None\n",
    "    if gpu is not None:\n",
    "        device_string='/device:GPU:{0}'.format(gpu)\n",
    "    return tf.device(device_string)\n",
    "\n",
    "def _init_keras():\n",
    "    # Setup the session to dynamically allocate memory\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    K.set_session(session)\n",
    "\n",
    "# Init the default session to be used by Keras\n",
    "_init_keras()\n",
    "def load_data():\n",
    "    rootdir = os.path.abspath(os.getcwd())\n",
    "    datadir = os.path.join(os.path.join(rootdir, \"data\"), \"MR\")\n",
    "    pos_pattern = os.path.join(os.path.join(datadir,\"positive_images\"), \"*.jpg\")\n",
    "    neg_pattern = os.path.join(os.path.join(datadir,\"negative_images\"), \"*.jpg\")\n",
    "    pos_filenames = list(glob.glob(pos_pattern))\n",
    "    neg_filenames = list(glob.glob(neg_pattern))\n",
    "\n",
    "    pos_images = [matplotlib.image.imread(fname) for fname in pos_filenames]\n",
    "    neg_images = [matplotlib.image.imread(fname) for fname in neg_filenames]\n",
    "    X = np.vstack([np.array(pos_images, dtype=np.float32), np.array(neg_images, dtype=np.float32)])\n",
    "    y = np.array([1]*len(pos_images) + [0]*len(neg_images), dtype=np.int32)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "def crossentropy_logits(y_true, y_pred):\n",
    "    return K.categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "\n",
    "def build_conv_model():\n",
    "    # Images are 1 channel and 32x32\n",
    "    input = Input(shape=(32, 32, 1))\n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(input)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(2)(x)\n",
    "    return Model(input, x)\n",
    "\n",
    "def to_tensors(X, y):\n",
    "    return X[:, :, :, np.newaxis], to_categorical(y, num_classes=2)\n",
    "\n",
    "def plot_patches(X, y, y_true=None, to_plot=None):    \n",
    "    to_plot = to_plot or len(X)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    for i in range(to_plot):\n",
    "        plt.subplot(1, to_plot, i+1)\n",
    "        plt.imshow(X[i].reshape((32, 32)), interpolation='nearest', cmap='gray')\n",
    "        plt.text(0, 0, y[i], color='black', \n",
    "                 bbox=dict(facecolor='white', alpha=1))\n",
    "        if y_true is not None:\n",
    "            plt.text(0, 32, y_true[i], color='black', \n",
    "                     bbox=dict(facecolor='white', alpha=1))\n",
    "            \n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "class LogCallback(Callback):            \n",
    "    def on_epoch_end(self, epoch, logs=None):                                        \n",
    "        print(\"{}: L: {:.7} A: {:.7} VL: {:.7} VA: {:.7}\".format(epoch,                                                                            \n",
    "                                                                 logs['loss'], \n",
    "                                                                 logs['acc'], \n",
    "                                                                 logs['val_loss'], \n",
    "                                                                 logs['val_acc']))    \n",
    "    \n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "plot_patches(X_train, y_train, to_plot=10)\n",
    "\n",
    "X_train, y_train = to_tensors(X_train, y_train)\n",
    "X_test, y_test = to_tensors(X_test, y_test)\n",
    "\n",
    "with get_device():\n",
    "    model = build_conv_model()\n",
    "\n",
    "    optimizer = Adam()\n",
    "    model.compile(optimizer=optimizer, loss = crossentropy_logits, metrics=['accuracy'])\n",
    "\n",
    "    n_train = -1\n",
    "    logs = model.fit(X_train[:n_train], y_train[:n_train], \n",
    "                     validation_split=0.3, \n",
    "                     epochs=5,\n",
    "                     verbose=0,\n",
    "                     callbacks=[LogCallback()])\n",
    "plt.plot(logs.history['acc'])\n",
    "plt.plot(logs.history['val_acc'])\n",
    "plt.show()\n",
    "\n",
    "with get_device():\n",
    "    # Predict on test data\n",
    "    y_proba_test = model.predict(X_test)\n",
    "y_pred_test = np.argmax(y_proba_test, axis=-1)\n",
    "y_true = np.argmax(y_test, axis=-1)\n",
    "errors = y_pred_test != y_true\n",
    "# Compute the accuracy    \n",
    "print(\"Accuracy: {}\".format(1.0-np.mean(errors)))\n",
    "\n",
    "# Plot the first examples.\n",
    "to_evaluate = 15\n",
    "X_eval = X_test[:to_evaluate]    \n",
    "y_eval = y_pred_test[:to_evaluate]\n",
    "plot_patches(X_eval, y_eval, y_true=y_true[:to_evaluate])\n",
    "\n",
    "# Plot the first error examples\n",
    "X_eval = X_test[np.where(errors)][:to_evaluate]\n",
    "y_eval = y_pred_test[np.where(errors)][:to_evaluate]\n",
    "plot_patches(X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
