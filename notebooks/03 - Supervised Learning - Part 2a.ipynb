{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P2vTTiHFriI1"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYfuABDMriJF"
   },
   "source": [
    "# Supervised Learning Part 2a -- Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PiLonLoriJH"
   },
   "source": [
    "To visualize the workings of machine learning algorithms, it is often helpful to study two-dimensional or one-dimensional data, that is data with only one or two features. While in practice, datasets usually have many more features, it is hard to plot high-dimensional data on two-dimensional screens.\n",
    "\n",
    "We will illustrate some very simple examples before we move on to more \"real world\" data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GX_y_jXDriJK"
   },
   "source": [
    "First, we will look at a two class classification problem in two dimensions. We use the synthetic data generated by the ``make_blobs`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wC8mGYdrriJM"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(centers=2, random_state=0)\n",
    "\n",
    "print('X ~ n_samples x n_features:', X.shape)\n",
    "print('y ~ n_samples:', y.shape)\n",
    "\n",
    "print('\\nFirst 5 samples:\\n', X[:5, :])\n",
    "print('\\nFirst 5 labels:', y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbkLsVUjriJW"
   },
   "source": [
    "As the data is two-dimensional, we can plot each sample as a point in a two-dimensional coordinate system, with the first feature being the x-axis and the second feature being the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npFIFbfdriJZ"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], \n",
    "            c='blue', s=40, label='0')\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], \n",
    "            c='red', s=40, label='1', marker='s')\n",
    "\n",
    "plt.xlabel('first feature')\n",
    "plt.ylabel('second feature')\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we need to split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xs82Gl0mriJq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=1234,\n",
    "                                                    stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \n",
    "            c='blue', s=40, label='0')\n",
    "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \n",
    "            c='red', s=40, label='1', marker='s')\n",
    "\n",
    "plt.xlabel('first feature')\n",
    "plt.ylabel('second feature')\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQVysqpHriJ3"
   },
   "source": [
    "We import the `LogisticRegression` class and instatiate a new object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxPfM8UBriJ4"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7nSe6t8KriKa"
   },
   "source": [
    "To built the model from our data, that is to learn how to classify new points, we call the ``fit`` function with the training data, and the corresponding training labels (the desired output for the training data point):\n",
    "\n",
    "**NOTE: You may get a warning about the default solver, this can safely be ignored.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xjyrb_CriKb"
   },
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGmBM8KhriKl"
   },
   "source": [
    "We can then apply the model to unseen data and use the model to predict the estimated outcome using the ``predict`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AkeeoeeSriKn"
   },
   "outputs": [],
   "source": [
    "prediction = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k4M0-GgdriKw"
   },
   "source": [
    "We can compare these against the true labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fiXozs-OriKz"
   },
   "outputs": [],
   "source": [
    "print(prediction)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tSBQmvAZriK7"
   },
   "source": [
    "We can evaluate our classifier quantitatively by measuring what fraction of predictions is correct. This is called **accuracy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPSf_45oriK9"
   },
   "outputs": [],
   "source": [
    "np.mean(prediction == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "guaiiyXpriLH"
   },
   "source": [
    "Again, scikit-learn provides an easy way to evaluate the prediction quantitatively using the ``score`` method.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3uo_jUvsriLK"
   },
   "outputs": [],
   "source": [
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "carh5lUMriLZ"
   },
   "source": [
    "It is often helpful to compare the generalization performance (on the test set) to the performance on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3132LO4lriLg"
   },
   "outputs": [],
   "source": [
    "classifier.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sRV3ZYHCriLq"
   },
   "source": [
    "LogisticRegression is a so-called linear model,\n",
    "that means it will create a decision that is linear in the input space. In 2d, this simply means it finds a line to separate the blue from the red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-BNXv8ZriLs"
   },
   "outputs": [],
   "source": [
    "def plot_2d_separator(classifier, X, fill=False, ax=None, eps=None):\n",
    "    if eps is None:\n",
    "        eps = X.std() / 2.\n",
    "    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n",
    "    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n",
    "    xx = np.linspace(x_min, x_max, 100)\n",
    "    yy = np.linspace(y_min, y_max, 100)\n",
    "\n",
    "    X1, X2 = np.meshgrid(xx, yy)\n",
    "    X_grid = np.c_[X1.ravel(), X2.ravel()]\n",
    "    try:\n",
    "        decision_values = classifier.decision_function(X_grid)\n",
    "        levels = [0]\n",
    "        fill_levels = [decision_values.min(), 0, decision_values.max()]\n",
    "    except AttributeError:\n",
    "        # no decision_function\n",
    "        decision_values = classifier.predict_proba(X_grid)[:, 1]\n",
    "        levels = [.5]\n",
    "        fill_levels = [0, .5, 1]\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    if fill:\n",
    "        ax.contourf(X1, X2, decision_values.reshape(X1.shape),\n",
    "                    levels=fill_levels, colors=['blue', 'red'])\n",
    "    else:\n",
    "        ax.contour(X1, X2, decision_values.reshape(X1.shape), levels=levels,\n",
    "                   colors=\"black\")\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6u0XD2DriMD"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], \n",
    "            c='blue', s=40, label='0')\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], \n",
    "            c='red', s=40, label='1', marker='s')\n",
    "\n",
    "plt.xlabel(\"first feature\")\n",
    "plt.ylabel(\"second feature\")\n",
    "plot_2d_separator(classifier, X)\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkeCnlW3riMe"
   },
   "source": [
    "**Estimated parameters**: All the estimated model parameters are attributes of the estimator object ending by an underscore. Here, these are the coefficients and the offset of the line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "omJMSCiyriMg"
   },
   "outputs": [],
   "source": [
    "print(classifier.coef_)\n",
    "print(classifier.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Td2B38yKriMl"
   },
   "source": [
    "___\n",
    "## Exercise\n",
    "The example above was a very simple example with only two features. Logistic regression can work for data with a lot more dimensions as well. We will now create a classifier that can predict whether a breast cancer tumour is malignant or benign.\n",
    "\n",
    "Use the breast cancer data and create a logistic regression classifier.\n",
    "\n",
    "You can look at the simple example above for guidance.\n",
    "\n",
    "**NOTE: The plot_2d_separator will not work with this dataset!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XbcT8EG1riMn"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "bc_data = load_breast_cancer()\n",
    "print(bc_data.feature_names)\n",
    "\n",
    "X, y = bc_data.data, bc_data.target\n",
    "\n",
    "# We need to:\n",
    "# Split the data into training and test sets\n",
    "# Create a LogisticRegression object\n",
    "# Fit the regressor object to the training data\n",
    "# Evaluate the model on the test data\n",
    "\n",
    "# ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_decomp = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_idx = np.where(y==0)[0]\n",
    "pos_idx = np.where(y==1)[0]\n",
    "plt.plot(X_decomp[neg_idx,0], X_decomp[neg_idx, 1],'o', c='r')\n",
    "plt.plot(X_decomp[pos_idx,0], X_decomp[pos_idx, 1],'o', c='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_decomp = pca.transform(classifier.coef_)\n",
    "dir_v = np.asarray([1.0, (-coef_decomp[0,0] -classifier.intercept_)/coef_decomp[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(coef_decomp , dir_v) + classifier.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.arange(-50, 80, 100)\n",
    "x2 = x1*dir_v[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_idx = np.where(y==0)[0]\n",
    "pos_idx = np.where(y==1)[0]\n",
    "plt.plot(X_decomp[neg_idx,0], X_decomp[neg_idx, 1],'o', c='r')\n",
    "plt.plot(X_decomp[pos_idx,0], X_decomp[pos_idx, 1],'o', c='b')\n",
    "plt.plot(x1, x2, c='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r2ZcuAo4riMt"
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0wZhE_YriMu"
   },
   "source": [
    "## Another classifier: K Nearest Neighbors\n",
    "Another popular and easy to understand classifier is K nearest neighbors (kNN).  It has one of the simplest learning strategies: given a new, unknown observation, look up in your reference database which ones have the closest features and assign the predominant class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOh58vLwriMw"
   },
   "outputs": [],
   "source": [
    "# First reload our synthetic data\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_blobs(centers=2, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=1234,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xcjLqNn0riM1"
   },
   "source": [
    "The interface is exactly the same as for ``LogisticRegression above``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F8YOJzfWriM4"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8NZiwz8riM8"
   },
   "source": [
    "This time we set a parameter of the KNeighborsClassifier to tell it we only want to look at one nearest neighbor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HXbe_gQuriNA"
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p5ZfpinriNE"
   },
   "source": [
    "We fit the model with out training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vI2gjUabriNG"
   },
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d1zl_3UVriNO"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], \n",
    "            c='blue', s=40, label='0')\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], \n",
    "            c='red', s=40, label='1', marker='s')\n",
    "\n",
    "plt.xlabel(\"first feature\")\n",
    "plt.ylabel(\"second feature\")\n",
    "plot_2d_separator(knn, X)\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgDACPCLriNW"
   },
   "outputs": [],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFeKQGXEriNa"
   },
   "source": [
    "Now train a KNN-classifier for the breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LNRyGzJzriNc"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "bc_data = load_breast_cancer()\n",
    "print(bc_data.feature_names)\n",
    "\n",
    "X, y = bc_data.data, bc_data.target\n",
    "\n",
    "# We need to:\n",
    "# Split the data into training and test sets\n",
    "# Create a KNeighborsClassifier object (how many neighbours do you want to use?)\n",
    "# Fit the regressor object to the training data\n",
    "# Evaluate the model on the test data\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJo4TDONriNh"
   },
   "source": [
    "___\n",
    "## Exercise\n",
    "Apply the KNeighborsClassifier to the ``iris`` dataset. Play with different values of the ``n_neighbors`` and observe how training and test score change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DHkuuBR4riNi"
   },
   "outputs": [],
   "source": [
    "# %load solutions/knn_with_diff_k.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4a5xRx28riNn"
   },
   "source": [
    "On Google Colab, visit [knn_with_diff_k.py](https://github.com/fordanic/cmiv-ai-course/blob/master/notebooks/solutions/knn_with_diff_k.py) and manually copy the content of the solution and paste to the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1nGqUVJ4riNp"
   },
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "03 - Supervised Learning - Part 2a.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
