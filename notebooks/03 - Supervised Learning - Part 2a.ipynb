{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03 - Supervised Learning - Part 2a.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "P2vTTiHFriI1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bYfuABDMriJF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Supervised Learning Part 2a -- Classification"
      ]
    },
    {
      "metadata": {
        "id": "0PiLonLoriJH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To visualize the workings of machine learning algorithms, it is often helpful to study two-dimensional or one-dimensional data, that is data with only one or two features. While in practice, datasets usually have many more features, it is hard to plot high-dimensional data on two-dimensional screens.\n",
        "\n",
        "We will illustrate some very simple examples before we move on to more \"real world\" data sets."
      ]
    },
    {
      "metadata": {
        "id": "GX_y_jXDriJK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, we will look at a two class classification problem in two dimensions. We use the synthetic data generated by the ``make_blobs`` function."
      ]
    },
    {
      "metadata": {
        "id": "wC8mGYdrriJM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y = make_blobs(centers=2, random_state=0)\n",
        "\n",
        "print('X ~ n_samples x n_features:', X.shape)\n",
        "print('y ~ n_samples:', y.shape)\n",
        "\n",
        "print('\\nFirst 5 samples:\\n', X[:5, :])\n",
        "print('\\nFirst 5 labels:', y[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FbkLsVUjriJW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As the data is two-dimensional, we can plot each sample as a point in a two-dimensional coordinate system, with the first feature being the x-axis and the second feature being the y-axis."
      ]
    },
    {
      "metadata": {
        "id": "npFIFbfdriJZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], \n",
        "            c='blue', s=40, label='0')\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], \n",
        "            c='red', s=40, label='1', marker='s')\n",
        "\n",
        "plt.xlabel('first feature')\n",
        "plt.ylabel('second feature')\n",
        "plt.legend(loc='upper right');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q1g6cPWkriJo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Classification is a supervised task, and since we are interested in its performance on unseen data, we split our data into two parts:\n",
        "\n",
        "1. a training set that the learning algorithm uses to fit the model\n",
        "2. a test set to evaluate the generalization performance of the model\n",
        "\n",
        "The ``train_test_split`` function from the ``model_selection`` module does that for us -- we will use it to split a dataset into 75% training data and 25% test data.\n",
        "\n",
        "<img width=\"50%\" src='https://github.com/fordanic/cmiv-ai-course/blob/master/notebooks/figures/train_test_split_matrix.png?raw=1'/>\n"
      ]
    },
    {
      "metadata": {
        "id": "xs82Gl0mriJq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state=1234,\n",
        "                                                    stratify=y)\n",
        "\n",
        "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \n",
        "            c='blue', s=40, label='0')\n",
        "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \n",
        "            c='red', s=40, label='1', marker='s')\n",
        "\n",
        "plt.xlabel('first feature')\n",
        "plt.ylabel('second feature')\n",
        "plt.legend(loc='upper right');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tYYXFCDHriJy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The scikit-learn estimator API and Logistic Regresion"
      ]
    },
    {
      "metadata": {
        "id": "6diClR9-riJ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img width=\"50%\" src='https://github.com/fordanic/cmiv-ai-course/blob/master/notebooks/figures/supervised_workflow.png?raw=1'/>"
      ]
    },
    {
      "metadata": {
        "id": "eQVysqpHriJ3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Every algorithm is exposed in scikit-learn via an ''Estimator'' object. (All models in scikit-learn have a very consistent interface). For instance, we first import the logistic regression class."
      ]
    },
    {
      "metadata": {
        "id": "LxPfM8UBriJ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qPuT_nErriKC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we instantiate the estimator object."
      ]
    },
    {
      "metadata": {
        "id": "0Gf4zei1riKE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier = LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "52WxkhwnriKL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wpqHO1YKriKT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7nSe6t8KriKa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To built the model from our data, that is to learn how to classify new points, we call the ``fit`` function with the training data, and the corresponding training labels (the desired output for the training data point):"
      ]
    },
    {
      "metadata": {
        "id": "9xjyrb_CriKb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sQHpxRhDriKi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "(Some estimator methods such as `fit` return `self` by default. Thus, after executing the code snippet above, you will see the default parameters of this particular instance of `LogisticRegression`. Another way of retrieving the estimator's ininitialization parameters is to execute `classifier.get_params()`, which returns a parameter dictionary.)"
      ]
    },
    {
      "metadata": {
        "id": "PGmBM8KhriKl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can then apply the model to unseen data and use the model to predict the estimated outcome using the ``predict`` method:"
      ]
    },
    {
      "metadata": {
        "id": "AkeeoeeSriKn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prediction = classifier.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k4M0-GgdriKw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can compare these against the true labels:"
      ]
    },
    {
      "metadata": {
        "id": "fiXozs-OriKz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(prediction)\n",
        "print(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tSBQmvAZriK7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can evaluate our classifier quantitatively by measuring what fraction of predictions is correct. This is called **accuracy**:"
      ]
    },
    {
      "metadata": {
        "id": "YPSf_45oriK9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.mean(prediction == y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "guaiiyXpriLH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There is also a convenience function , ``score``, that all scikit-learn classifiers have to compute this directly from the test data:\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "3uo_jUvsriLK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "carh5lUMriLZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is often helpful to compare the generalization performance (on the test set) to the performance on the training set:"
      ]
    },
    {
      "metadata": {
        "id": "3132LO4lriLg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier.score(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sRV3ZYHCriLq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LogisticRegression is a so-called linear model,\n",
        "that means it will create a decision that is linear in the input space. In 2d, this simply means it finds a line to separate the blue from the red:"
      ]
    },
    {
      "metadata": {
        "id": "U-BNXv8ZriLs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_2d_separator(classifier, X, fill=False, ax=None, eps=None):\n",
        "    if eps is None:\n",
        "        eps = X.std() / 2.\n",
        "    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n",
        "    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n",
        "    xx = np.linspace(x_min, x_max, 100)\n",
        "    yy = np.linspace(y_min, y_max, 100)\n",
        "\n",
        "    X1, X2 = np.meshgrid(xx, yy)\n",
        "    X_grid = np.c_[X1.ravel(), X2.ravel()]\n",
        "    try:\n",
        "        decision_values = classifier.decision_function(X_grid)\n",
        "        levels = [0]\n",
        "        fill_levels = [decision_values.min(), 0, decision_values.max()]\n",
        "    except AttributeError:\n",
        "        # no decision_function\n",
        "        decision_values = classifier.predict_proba(X_grid)[:, 1]\n",
        "        levels = [.5]\n",
        "        fill_levels = [0, .5, 1]\n",
        "\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    if fill:\n",
        "        ax.contourf(X1, X2, decision_values.reshape(X1.shape),\n",
        "                    levels=fill_levels, colors=['blue', 'red'])\n",
        "    else:\n",
        "        ax.contour(X1, X2, decision_values.reshape(X1.shape), levels=levels,\n",
        "                   colors=\"black\")\n",
        "    ax.set_xlim(x_min, x_max)\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W6u0XD2DriMD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], \n",
        "            c='blue', s=40, label='0')\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], \n",
        "            c='red', s=40, label='1', marker='s')\n",
        "\n",
        "plt.xlabel(\"first feature\")\n",
        "plt.ylabel(\"second feature\")\n",
        "plot_2d_separator(classifier, X)\n",
        "plt.legend(loc='upper right');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EkeCnlW3riMe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Estimated parameters**: All the estimated model parameters are attributes of the estimator object ending by an underscore. Here, these are the coefficients and the offset of the line:"
      ]
    },
    {
      "metadata": {
        "id": "omJMSCiyriMg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(classifier.coef_)\n",
        "print(classifier.intercept_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Td2B38yKriMl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "___\n",
        "## Exercise\n",
        "The example above was a very simple example with only two features. Logistic regression can work for data with a lot more dimensions as well. We will now create a classifier that can predict whether a breast cancer tumour is malignant or benign.\n",
        "\n",
        "Use the breast cancer data and create a logistic regression classifier.\n",
        "\n",
        "You can look at the simple example above for guidance.\n",
        "\n",
        "NOTE: The plot_2d_separator will not work with this dataset!"
      ]
    },
    {
      "metadata": {
        "id": "XbcT8EG1riMn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "bc_data = load_breast_cancer()\n",
        "print(bc_data.DESCR)\n",
        "\n",
        "X, y = bc_data.data, bc_data.target\n",
        "\n",
        "# ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r2ZcuAo4riMt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "___"
      ]
    },
    {
      "metadata": {
        "id": "A0wZhE_YriMu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Another classifier: K Nearest Neighbors\n",
        "Another popular and easy to understand classifier is K nearest neighbors (kNN).  It has one of the simplest learning strategies: given a new, unknown observation, look up in your reference database which ones have the closest features and assign the predominant class."
      ]
    },
    {
      "metadata": {
        "id": "zOh58vLwriMw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# First reload our synthetic data\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_blobs(centers=2, random_state=0)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state=1234,\n",
        "                                                    stratify=y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xcjLqNn0riM1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The interface is exactly the same as for ``LogisticRegression above``."
      ]
    },
    {
      "metadata": {
        "id": "F8YOJzfWriM4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n8NZiwz8riM8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This time we set a parameter of the KNeighborsClassifier to tell it we only want to look at one nearest neighbor:"
      ]
    },
    {
      "metadata": {
        "id": "HXbe_gQuriNA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6p5ZfpinriNE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We fit the model with out training data"
      ]
    },
    {
      "metadata": {
        "id": "vI2gjUabriNG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "knn.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d1zl_3UVriNO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], \n",
        "            c='blue', s=40, label='0')\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], \n",
        "            c='red', s=40, label='1', marker='s')\n",
        "\n",
        "plt.xlabel(\"first feature\")\n",
        "plt.ylabel(\"second feature\")\n",
        "plot_2d_separator(knn, X)\n",
        "plt.legend(loc='upper right');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xgDACPCLriNW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "knn.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dFeKQGXEriNa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now train a KNN-classifier for the breast cancer dataset."
      ]
    },
    {
      "metadata": {
        "id": "LNRyGzJzriNc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "bc_data = load_breast_cancer()\n",
        "print(bc_data.DESCR)\n",
        "\n",
        "X, y = bc_data.data, bc_data.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dJo4TDONriNh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "___\n",
        "## Exercise\n",
        "Apply the KNeighborsClassifier to the ``iris`` dataset. Play with different values of the ``n_neighbors`` and observe how training and test score change."
      ]
    },
    {
      "metadata": {
        "id": "DHkuuBR4riNi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# %load solutions/knn_with_diff_k.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4a5xRx28riNn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On Google Colab, visit [knn_with_diff_k.py](https://github.com/fordanic/cmiv-ai-course/blob/master/notebooks/solutions/knn_with_diff_k.py) and manually copy the content of the solution and paste to the cell above."
      ]
    },
    {
      "metadata": {
        "id": "1nGqUVJ4riNp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "___"
      ]
    }
  ]
}