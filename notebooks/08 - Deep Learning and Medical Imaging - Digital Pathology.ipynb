{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6vYAeWDBCxw"
   },
   "source": [
    "# Deep Learning and Medical Imaging - Digital Pathology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HqfaN8OeBCxy"
   },
   "source": [
    "In this exercise, we will solve the same problem as before, i.e. we have images corresponding to 2 categories and we would like to train a classifier that can separate between the two sets of images. However, in this case we are looking at HE-stained digital pathology images (i.e. RGB images instead of grayscale images), where we will try to separate between epithelium and stroma regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KGbhPuILBCxz"
   },
   "source": [
    "## Imports, sampling and data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9KCrVn5CCQo8"
   },
   "source": [
    "We start by downloading the data that we will use. \n",
    "\n",
    "The data is downloaded from http://www.andrewjanowczyk.com/deep-learning/. The datasets available on this webpage were published in conjunction with a tutorial on deep learning for digital pathology (http://www.jpathinformatics.org/text.asp?2016/7/1/29/186902). Feel free to check this out if you want to explore the topic of deep learning and digital pathology any further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BH4haP2RBCx0"
   },
   "outputs": [],
   "source": [
    "# Run the below lines to download and unpack the data when running in Colab.\n",
    "# Remember to uncomment.\n",
    "!wget -O epi.tar https://andrewjanowczyk.com/wp-static/epi.tgz --no-check-certificate\n",
    "!mkdir epi\n",
    "!tar -xf epi.tar -C epi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rmI6bpuLCsg4"
   },
   "source": [
    "This time, the data has not been sampled, hence we need to create the samples (image patches) ourselves. The original images are located in the folder \"epi\" and the masks for epithelium in the subfolder \"masks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M_xANPAcC3mS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "folder = \"epi\"\n",
    "\n",
    "# Find all images and mask images\n",
    "orig_images = glob.glob(os.path.join(folder,\"*.tif\"))\n",
    "mask_images = glob.glob(os.path.join(folder,\"masks\",\"*.png\"))\n",
    "\n",
    "# Not all images have mask images, so we need to find those that do\n",
    "orig_images_with_mask = list()\n",
    "orig_image_names = [os.path.splitext(os.path.basename(fname))[0] for fname in orig_images]\n",
    "mask_image_names = set([os.path.splitext(os.path.basename(fname))[0].replace(\"_mask\",\"\") for fname in mask_images])\n",
    "for orig_image in orig_images:\n",
    "    orig_image_name = os.path.splitext(os.path.basename(orig_image))[0]\n",
    "    if orig_image_name in mask_image_names:\n",
    "        mask_image = os.path.join(folder,\"masks\",orig_image_name + \"_mask.png\")\n",
    "        orig_images_with_mask.append((orig_image, mask_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tH8kZfw-Hpjs"
   },
   "source": [
    "Before we start sampling, let's have a look at the data so we understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pttX9boyHu_r"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, segmentation\n",
    "\n",
    "number_of_images_to_display = 3\n",
    "\n",
    "# For each original image with mask\n",
    "for (orig_image, mask_image) in orig_images_with_mask[0:number_of_images_to_display]:\n",
    "    print(\"Displaying:\", orig_image,\"and\",mask_image)\n",
    "    org_im = io.imread(orig_image)\n",
    "    mask_im = io.imread(mask_image)\n",
    "    mask_edges = segmentation.mark_boundaries(org_im, mask_im)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(mask_edges)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(mask_im, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fuR6Ph2KIwFM"
   },
   "source": [
    "Time to sample the data.\n",
    "\n",
    "In the code below, you can control the number of samples per images, and also the size of the samples. Note, if changing the size of the data, you need  to ensure that you update subsequent parts of the notebook that assumes the size of 64.\n",
    "\n",
    "In this exercise, we will also split the data into separate training and test sets right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YoqRt4AnO8PN"
   },
   "outputs": [],
   "source": [
    "# Simple function to sample an image\n",
    "def sample_and_save(image, pos, sample_size, folder, sample_counter):\n",
    "    for xy in pos:\n",
    "        sample_counter = sample_counter + 1\n",
    "        x = xy[0]\n",
    "        y = xy[1]\n",
    "        sample = image[x-sample_size//2:x+sample_size//2,\n",
    "                       y-sample_size//2:y+sample_size//2,\n",
    "                       :]\n",
    "        io.imsave(folder + str(sample_counter).zfill(5) + \".png\", sample)\n",
    "\n",
    "    return sample_counter  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XQCEpMX4Ps3m"
   },
   "source": [
    "Run the cell below once only, as it might take considerable time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SyjiLIOFbVd"
   },
   "outputs": [],
   "source": [
    "from skimage.morphology import erosion, square\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "# Parameters for sampling\n",
    "sample_size = 64\n",
    "samples_per_image = 10\n",
    "\n",
    "# Set and create folders for samples\n",
    "folder = \"epi\"\n",
    "output_folder = os.path.join(folder,\"samples\")\n",
    "if os.path.exists(output_folder):\n",
    "    shutil.rmtree(output_folder)\n",
    "os.mkdir(output_folder)\n",
    "for sample_category in [\"train\",\"test\"]:\n",
    "    os.mkdir(os.path.join(output_folder,sample_category))\n",
    "    os.mkdir(os.path.join(output_folder,sample_category,\"positive\"))\n",
    "    os.mkdir(os.path.join(output_folder,sample_category,\"negative\"))\n",
    "  \n",
    "# Shuffle our cases\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(orig_images_with_mask)\n",
    "\n",
    "# Start sampling patches for training, but we'll change sample category \n",
    "# as we move along based upon our counter\n",
    "counter = 0\n",
    "sample_category = \"train\"\n",
    "# Set sample counters\n",
    "pos_sample_counter = 0\n",
    "neg_sample_counter = 0\n",
    "# For each original image with mask\n",
    "for (orig_image, mask_image) in orig_images_with_mask:\n",
    "    counter = counter + 1\n",
    "    \n",
    "    if counter == 35:\n",
    "        # Change to test\n",
    "        sample_category = \"test\"\n",
    "        # Reset sample counters\n",
    "        pos_sample_counter = 0\n",
    "        neg_sample_counter = 0\n",
    "    \n",
    "    print(\"Processing\", orig_image,\"and\",mask_image,\"to create\",sample_category)\n",
    "    org_im = io.imread(orig_image)\n",
    "    mask_im = io.imread(mask_image)\n",
    "    \n",
    "    # Create foreground and background masks\n",
    "    fg_im = np.zeros(mask_im.shape)\n",
    "    bg_im = np.zeros(mask_im.shape)\n",
    "    fg_im[mask_im == 255] = 1\n",
    "    bg_im[mask_im != 255] = 1\n",
    "    \n",
    "    # Erode masks\n",
    "    fg_im_eroded = erosion(fg_im, square(sample_size//2))\n",
    "    bg_im_eroded = erosion(bg_im, square(sample_size//2))\n",
    "    \n",
    "    # Remove masks close to the edges of images\n",
    "    im_size = org_im.shape[0]\n",
    "    fg_im_eroded[:sample_size//2,:] = 0\n",
    "    fg_im_eroded[im_size-sample_size//2:,:] = 0\n",
    "    fg_im_eroded[:,:sample_size//2] = 0\n",
    "    fg_im_eroded[:,im_size-sample_size//2:] = 0\n",
    "    bg_im_eroded[:sample_size//2,:] = 0\n",
    "    bg_im_eroded[im_size-sample_size//2:,:] = 0\n",
    "    bg_im_eroded[:,:sample_size//2] = 0\n",
    "    bg_im_eroded[:,im_size-sample_size//2:] = 0\n",
    "\n",
    "    # Find indices to sample from\n",
    "    pos_ind = np.where(fg_im_eroded==1)\n",
    "    pos_ind = np.concatenate((np.expand_dims(pos_ind[0], axis=1),\n",
    "                              np.expand_dims(pos_ind[1], axis=1)), axis=1)\n",
    "    neg_ind = np.where(bg_im_eroded==1)\n",
    "    neg_ind = np.concatenate((np.expand_dims(neg_ind[0], axis=1),\n",
    "                              np.expand_dims(neg_ind[1], axis=1)), axis=1)\n",
    "    \n",
    "    # Shuffle indices\n",
    "    np.random.seed(123)\n",
    "    np.random.shuffle(pos_ind)\n",
    "    np.random.seed(321)\n",
    "    np.random.shuffle(neg_ind)\n",
    "\n",
    "    # Get indices to use\n",
    "    pos_ind = pos_ind[0:samples_per_image+1,:]\n",
    "    neg_ind = neg_ind[0:samples_per_image+1,:]\n",
    "\n",
    "    # Sample the images\n",
    "    pos_sample_counter = sample_and_save(org_im, pos_ind, sample_size, \n",
    "                                         os.path.join(output_folder, sample_category, \"positive\", \"pos_\"), \n",
    "                                         pos_sample_counter)\n",
    "    neg_sample_counter = sample_and_save(org_im, neg_ind, sample_size, \n",
    "                                         os.path.join(output_folder, sample_category, \"negative\", \"neg_\"), \n",
    "                                         neg_sample_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UCnqU0BAPgtu"
   },
   "source": [
    "No need to worry about the warnings that are produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OuZJwAgNJPGJ"
   },
   "source": [
    "We list the samples we have available and split them into train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-m_D2miBCx4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(datadir, sample_category, apply_train_test_split=True):\n",
    "    pos_filenames = glob.glob(os.path.join(datadir,sample_category,\"positive\", \"*.png\"))\n",
    "    neg_filenames = glob.glob(os.path.join(datadir,sample_category,\"negative\", \"*.png\"))\n",
    "    \n",
    "    pos_images = [matplotlib.image.imread(fname) for fname in pos_filenames]\n",
    "    neg_images = [matplotlib.image.imread(fname) for fname in neg_filenames]\n",
    "    \n",
    "    X = np.vstack([np.array(pos_images, dtype=np.float32), np.array(neg_images, dtype=np.float32)])\n",
    "    y = np.array([1]*len(pos_images) + [0]*len(neg_images), dtype=np.int32)\n",
    "    \n",
    "    if apply_train_test_split:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                            test_size=0.2, \n",
    "                                                            stratify=y,\n",
    "                                                            random_state=123)\n",
    "    \n",
    "        return (X_train, y_train), (X_test, y_test)\n",
    "    else:\n",
    "        p = np.random.permutation(len(X))\n",
    "        return (X[p], y[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_jDeOaAwBCx6"
   },
   "outputs": [],
   "source": [
    "# This will load data from disk and cache it.\n",
    "(X_train, y_train), (X_val, y_val) = load_data(\"epi/samples\", \"train\")\n",
    "(X_test, y_test) = load_data(\"epi/samples\", \"test\", apply_train_test_split=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M9Jj8fKiBCx8"
   },
   "source": [
    "Check the size of the training and test sets, as well as the dimension of each array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pRxtQanzBCx-"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WuG1UyZOBCyA"
   },
   "source": [
    "We can visualize the image patches by plotting some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1JYzugdUBCyB"
   },
   "outputs": [],
   "source": [
    "def plot_patches(X, y, y_true=None, to_plot=None):    \n",
    "    to_plot = to_plot or len(X)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    for i in range(to_plot):\n",
    "        plt.subplot(1, to_plot, i+1)\n",
    "        plt.imshow(X[i].reshape((64, 64, 3)), interpolation='nearest')\n",
    "        plt.text(0, 0, y[i], color='black', \n",
    "                 bbox=dict(facecolor='white', alpha=1))\n",
    "        if y_true is not None:\n",
    "            plt.text(0, 32, y_true[i], color='black', \n",
    "                     bbox=dict(facecolor='white', alpha=1))\n",
    "            \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PHbvNYZaBCyD"
   },
   "outputs": [],
   "source": [
    "print(\"Training samples\")\n",
    "plot_patches(X_train, y_train, to_plot=10)\n",
    "print(\"Validation samples\")\n",
    "plot_patches(X_val, y_val, to_plot=10)\n",
    "print(\"Test samples\")\n",
    "plot_patches(X_test, y_test, to_plot=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nrAfCoxqRXik"
   },
   "source": [
    "To get started, we will use exactly the same approach as last time, but adjusted for somewhat large image patches and the fact that they are RGB images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hY27HZaTp-uy",
    "tags": []
   },
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First we need to run some code for support, you do not need to understand it*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CTsGHLYMqFaH"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation, BatchNormalization, Dropout, Dense, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_device():\n",
    "    device_string = '/cpu:0'\n",
    "    # Set to None to avoid using a GPU\n",
    "    gpu=0\n",
    "    if gpu is not None:\n",
    "        device_string='/device:GPU:{0}'.format(gpu)\n",
    "    return tf.device(device_string)\n",
    "\n",
    "# Define helper functions\n",
    "def to_tensors(X, y):\n",
    "    return X[:, :, :, :], to_categorical(y, num_classes=2)\n",
    "\n",
    "class LogCallback(Callback):            \n",
    "    def on_epoch_end(self, epoch, logs=None):                                        \n",
    "        print(\"{}: L: {:.7} A: {:.7} VL: {:.7} VA: {:.7}\".format(epoch,                                                                            \n",
    "                                                                 logs['loss'], \n",
    "                                                                 logs['accuracy'], \n",
    "                                                                 logs['val_loss'], \n",
    "                                                                 logs['val_accuracy'])) \n",
    "\n",
    "def crossentropy_logits(y_true, y_pred):\n",
    "    return K.categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "  \n",
    "def to_tensors(X, y):\n",
    "    # Convert X into a 4D tensor and y into a 2D tensor with 1-hot encoding\n",
    "    return X[:, :, :, :], to_categorical(y, num_classes=2)\n",
    "\n",
    "def Conv(n):\n",
    "    return Conv2D(n, (3, 3), padding=\"same\", activation='relu')\n",
    "\n",
    "def ConvMax(n):\n",
    "    return lambda x: MaxPooling2D(pool_size=(2, 2))(Conv(n)(x))\n",
    "\n",
    "real_dense = Dense\n",
    "def Dense(n):\n",
    "    return real_dense(n, activation='relu')\n",
    "\n",
    "def build_cnn_model(cnn_backbone, mlp_classifier=[]):\n",
    "    # Images are 3 channel and 64x64\n",
    "    input = Input(shape=(64, 64, 3))\n",
    "    x = input\n",
    "    for layer in cnn_backbone:\n",
    "        x = layer(x)\n",
    "    x = Flatten()(x)\n",
    "    for layer in mlp_classifier:\n",
    "        x = layer(x)\n",
    "    x = real_dense(2)(x)\n",
    "    return Model(input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "raKDuK4SBCyF",
    "tags": []
   },
   "source": [
    "Load data (only needed once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eIN9fHDotG4s"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "(X_train, y_train), (X_val, y_val) = load_data(\"epi/samples\", \"train\")\n",
    "(X_test, y_test) = load_data(\"epi/samples\", \"test\", apply_train_test_split=False)\n",
    "\n",
    "# Plot example data\n",
    "print(\"Example patches of epithelium and stroma\")\n",
    "plot_patches(X_train, y_train, to_plot=10)\n",
    "\n",
    "# Convert to expected format\n",
    "X_train, y_train = to_tensors(X_train, y_train)\n",
    "X_val, y_val = to_tensors(X_val, y_val)\n",
    "X_test, y_test = to_tensors(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model we need to:\n",
    "- Create the model\n",
    "- Decide on a loss function\n",
    "- Iteratively optimize the loss with respect to the model parameters\n",
    "- (Visualize the training and result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DyXOkI-OQsKO"
   },
   "outputs": [],
   "source": [
    "def run_experiment(model, optimizer, epochs, datagen=None):\n",
    "    # Run training\n",
    "    with get_device():\n",
    "        model.compile(optimizer=optimizer, loss = crossentropy_logits, metrics=['accuracy'])\n",
    "\n",
    "        n_train = -1\n",
    "        if (datagen is None):\n",
    "            logs = model.fit(X_train[:n_train], y_train[:n_train], \n",
    "                            validation_data=(X_val, y_val), \n",
    "                            epochs=epochs,\n",
    "                            verbose=0, callbacks=[LogCallback()])\n",
    "        else:\n",
    "            logs = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                             steps_per_epoch=len(X_train) // 32,\n",
    "                             validation_data=(X_val, y_val),\n",
    "                             epochs=epochs,\n",
    "                             verbose=0, callbacks=[LogCallback()])\n",
    "\n",
    "    plt.plot(logs.history['accuracy'], c='r', label='Train')\n",
    "    plt.plot(logs.history['val_accuracy'], c='g', label='Validation')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Predict on test data\n",
    "    y_proba_test = model.predict(X_test)\n",
    "    y_pred_test = np.argmax(y_proba_test, axis=-1)\n",
    "    y_true = np.argmax(y_test, axis=-1)\n",
    "    errors = y_pred_test != y_true\n",
    "\n",
    "    # Compute the accuracy    \n",
    "    print(\"Accuracy: {}\".format(1.0-np.mean(errors)))\n",
    "\n",
    "    # Plot the first examples\n",
    "    print(\"Some predicted examples\")\n",
    "    to_evaluate = 15\n",
    "    X_eval = X_test[:to_evaluate]    \n",
    "    y_eval = y_pred_test[:to_evaluate]\n",
    "    plot_patches(X_eval, y_eval, y_true=y_true[:to_evaluate])\n",
    "\n",
    "    # Plot the first error examples\n",
    "    print(\"Some false predictions\")\n",
    "    X_eval = X_test[np.where(errors)][:to_evaluate]\n",
    "    y_eval = y_pred_test[np.where(errors)][:to_evaluate]\n",
    "    plot_patches(X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "We can experiment with a number of parameters for our learning problem:\n",
    "\n",
    "Architecture:\n",
    "- Change the number of hidden nodes `ConvMax(128)`\n",
    "- Change the number of layers `[ConvMax(64), ConvMax(128)]`\n",
    "- Add normalization `[ConvMax(64), Dropout(0.2), ConvMax(128)]`\n",
    "- Add a MLP classifier at the end `[Dropout(0.2), Dense(256)]`\n",
    "\n",
    "Optimizer:\n",
    "- We can try another optimizer `optimizer = Adam()`\n",
    "\n",
    "Time:\n",
    "- We can increase the number of steps the optimizer takes (how far we run) `epochs=50`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "92FGZRNJtMVU"
   },
   "outputs": [],
   "source": [
    "model = build_cnn_model([ConvMax(64), ConvMax(64)], [Dropout(0.2), Dense(256)])\n",
    "\n",
    "optimizer = SGD(0.003)\n",
    "#optimizer = Adam()\n",
    "\n",
    "epochs = 25\n",
    "\n",
    "# Run the experiment with the above specified model and parameters.\n",
    "run_experiment(model, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4wendFbNEh-x"
   },
   "source": [
    "As we can note in this exercise, we don't have that many images to sample from, only about 40 images. Also, we haven't sampled our data that densely, i.e. just a few samples from each image. This means that we just have a few image samples to train on and with a limited variation. \n",
    "\n",
    "The natural approach to handle this would of course be to collect more images, annotate them and/or to sample more densely from the images that are available. Sometimes (often) this is not possible, and instead we need to use other techniques to create more training data. The most common approach is data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wMQLpsF3Fq23"
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSilgb06dx7V"
   },
   "source": [
    "A convolutional neural network that can robustly classify objects even if its placed in different orientations is said to have the property called invariance. More specifically, a CNN can be invariant to translation, viewpoint, size or illumination (or a combination of the above). However, in order to achieve this we need data with sufficient variation.\n",
    "![](https://github.com/fordanic/cmiv-ai-course/blob/master/notebooks/figures/data_augmentation.png?raw=1)\n",
    "This essentially is the premise of data augmentation. In the real world scenario, we may have a dataset of images taken in a limited set of conditions. But, our target application may exist in a variety of conditions, such as different orientation, location, scale, brightness etc. We account for these situations by training our neural network with additional synthetically modified data, taking into account these different variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AzAkxZ8gdx7W"
   },
   "source": [
    "___\n",
    "**Question**\n",
    "\n",
    "What are the variations that would be relevant to consider in a context of training an image classifier for radiology respectively pathology?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ob5Q8VrPIYd4"
   },
   "source": [
    "In the following, we'll make use of the class `ImageDataGenerator` in Keras to construct additional training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hgco7q4ndx7Z"
   },
   "source": [
    "First we run an example of offline data augmentation, i.e.we create additional data prior to running the training. In this case, we only create 20 new versions from the first image patch among our training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pDAMze54IlKJ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "if os.path.exists(\"preview\"):\n",
    "    shutil.rmtree(\"preview\")\n",
    "os.mkdir(\"preview\")\n",
    "\n",
    "# The class ImageDataGenerator is used to create additional data\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "x = X_train[0,:,:,:]\n",
    "x = np.transpose(x) # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "x = np.transpose(x)\n",
    "print(x.shape)\n",
    "\n",
    "# The .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "for batch in datagen.flow(X_train[0:1,:,:,:], batch_size=1, \n",
    "                          save_to_dir='preview', save_prefix='aug', save_format='png'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fq7witTWdx7f"
   },
   "source": [
    "Next, we can look at the data that we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S14gabpMMGir"
   },
   "outputs": [],
   "source": [
    "aug_filenames = glob.glob(os.path.join(\"preview\", \"*.png\"))\n",
    "\n",
    "aug_images = [matplotlib.image.imread(fname) for fname in aug_filenames]\n",
    "\n",
    "X_aug = np.vstack([np.array(aug_images, dtype=np.float32)])\n",
    "y_aug = np.array([1]*len(aug_images), dtype=np.int32)\n",
    "\n",
    "# Plot example data\n",
    "print(\"Example of augmented image samples\")\n",
    "plot_patches(X_aug, y_aug, to_plot=20)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AX6eeAahdx7l"
   },
   "source": [
    "However, for the actual training we will instead use oneline data augmentation, i.e. additional images will be created on the fly as we run the training. To do this we will make use of a data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KI-4DSYNcxk3"
   },
   "outputs": [],
   "source": [
    "# Create a data generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    # width_shift_range=0.2,\n",
    "    # height_shift_range=0.2,\n",
    "    # shear_range=0.2,\n",
    "    # zoom_range=0.2,\n",
    "    # horizontal_flip=True,\n",
    "    # vertical_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "# Create model, optimizer and amount of epochs as before\n",
    "model = build_cnn_model([ConvMax(64), ConvMax(64)], [Dropout(0.2), Dense(256)])\n",
    "\n",
    "optimizer = Adam()\n",
    "\n",
    "epochs = 25\n",
    "\n",
    "# Run the experiment with the above specified model and parameters.\n",
    "run_experiment(model, optimizer, epochs, datagen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**Questions**\n",
    "\n",
    "* What are good augmentations? Could you add too much?\n",
    "* Is it better to go back and sample more data? \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Actual Results\n",
    "Going back to slide level, what does these patch-level accuracies mean?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First some helper functions to create tiles from slide and to visualize the resulting predicitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import cv2\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def extract_tiles(org_im, mask_im, sample_size):\n",
    "    # Assume square patches, get coordinate of upper left pixel per tile\n",
    "    patch_size = org_im.shape[0]\n",
    "    image_size = patch_size - patch_size % sample_size\n",
    "    inds = np.asarray(range(0, image_size))[::sample_size]\n",
    "\n",
    "    # Extract tiles \n",
    "    tiles = []\n",
    "    mask_tiles = []\n",
    "    for y in inds:\n",
    "        for x in inds:\n",
    "            tile = org_im[x:x+sample_size,y:y+sample_size,:]/255\n",
    "            tiles.append(tile)\n",
    "            mask_tile = mask_im[x:x+sample_size,y:y+sample_size].copy()\n",
    "            mask_tile[mask_tile == 255] = 1\n",
    "            mask_tiles.append(mask_tile)\n",
    "\n",
    "    # Set center pixel in tile as label\n",
    "    labels = [mask_tile[sample_size//2, sample_size//2] for mask_tile in mask_tiles]\n",
    "    return tiles, labels\n",
    "\n",
    "\n",
    "def get_overlay(input_, sample_size):\n",
    "    num_tiles_x = num_tiles_y = int(math.sqrt(len(input_)))\n",
    "\n",
    "    input_mask = input_.reshape(num_tiles_x, num_tiles_y).T\n",
    "    input_mask = cv2.resize(input_mask, dsize=(sample_size*num_tiles_x, sample_size*num_tiles_y), interpolation=cv2.INTER_NEAREST )\n",
    "    return input_mask\n",
    "\n",
    "# Color map to visualize in/correctly predicted tiles\n",
    "newcmp = ListedColormap(np.array([[0, 0, 0, 0], [1, 1, 0, 1], [0, 1, 0, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a slide and run inference on its tiles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select slide, the final 8 slides were reserved for test\n",
    "(orig_image, mask_image) = orig_images_with_mask[-1]\n",
    "\n",
    "org_im = io.imread(orig_image)\n",
    "mask_im = io.imread(mask_image)\n",
    "\n",
    "tiles, labels = extract_tiles(org_im, mask_im, sample_size)\n",
    "y_test_slide = np.array(labels, dtype=np.int32)\n",
    "X_test_slide, y_test_slide = to_tensors(np.asarray(tiles), y_test_slide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction\n",
    "with get_device():\n",
    "    predictions = model.predict(X_test_slide)\n",
    "\n",
    "y_pred_test = np.argmax(predictions, axis=-1)\n",
    "y_true = np.argmax(y_test_slide, axis=-1)\n",
    "errors = y_pred_test != y_true\n",
    "\n",
    "print(\"Test accuracy for slide: {}\".format(1.0-np.mean(errors)))\n",
    "print(\"Number of tiles missclassified: {}\".format(errors.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "prediction_mask = get_overlay(y_pred_test, sample_size)\n",
    "label_mask = get_overlay(y_true, sample_size)\n",
    "mask_edges = segmentation.mark_boundaries(org_im, mask_im)\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(151)\n",
    "plt.imshow(mask_edges[:prediction_mask.shape[0], :prediction_mask.shape[1], ...])\n",
    "plt.title('Input')\n",
    "plt.axis('off')\n",
    "plt.subplot(152)\n",
    "plt.imshow(mask_im[:prediction_mask.shape[0], :prediction_mask.shape[1], ...], cmap='gray')\n",
    "plt.title('Segmentation mask')\n",
    "plt.axis('off')\n",
    "plt.subplot(153)\n",
    "plt.imshow(label_mask, cmap='gray')\n",
    "plt.title('Tile ground truth')\n",
    "plt.axis('off')\n",
    "plt.subplot(154)\n",
    "plt.imshow(prediction_mask, cmap='gray')\n",
    "plt.title('Tile predictions')\n",
    "plt.axis('off')\n",
    "plt.subplot(155)\n",
    "plt.imshow(mask_edges[:prediction_mask.shape[0], :prediction_mask.shape[1], ...])\n",
    "plt.imshow(label_mask + prediction_mask, alpha=0.5, cmap=newcmp)\n",
    "plt.title('Ground truth + prediction')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Discussion points:\n",
    "\n",
    "- How does the segmentation quality affect the results? \n",
    "- How much data can you extract from one slide? Sample dense (perhaps even overlapping), or sparse? Is all data 'worth' the same?\n",
    "- Pixel annotations are expensive (time and effort) - are there cheaper options?\n",
    "- How much data do you need? Number of patients? Slides? Patches? Multi-slite or not?\n",
    "- Do we need to train our model from scratch or can we use a pre-trained model? What about these so called 'foundation models'?\n",
    "- What type of user interface will show the results?\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "08 - Deep Learning and Medical Imaging - Digital Pathology.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
