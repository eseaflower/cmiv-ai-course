{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04 - Unsupervised Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6j8CgXEPG9pa",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xJ9P0IXAG9po"
      },
      "source": [
        "# Unsupervised Learning -- Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xOhaXv9eG9ps"
      },
      "source": [
        "Clustering is the task of gathering samples into groups of similar samples according to some predefined similarity or distance (dissimilarity) measure, such as the Euclidean distance.\n",
        "\n",
        "<img width=\"60%\" src='https://github.com/fordanic/cmiv-ai-course/blob/master/notebooks/figures/clustering.png?raw=1'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M-4e_UlQG9py"
      },
      "source": [
        "In this section we will explore a basic clustering task on some synthetic and real-world datasets.\n",
        "\n",
        "Here are some common applications of clustering algorithms:\n",
        "\n",
        "- Compression for data reduction\n",
        "- Summarizing data as a reprocessing step for recommender systems\n",
        "- Similarly:\n",
        "   - grouping related web news (e.g. Google News) and web search results\n",
        "   - grouping related stock quotes for investment portfolio management\n",
        "   - building customer profiles for market analysis\n",
        "- Building a code book of prototype samples for unsupervised feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qVogBvbuG9p3"
      },
      "source": [
        "Let's start by creating a simple, 2-dimensional, synthetic dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TAj6vw70G9p7",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y = make_blobs(random_state=42)\n",
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zz-XRMguG9qC",
        "colab": {}
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4ElTdb2AG9qF"
      },
      "source": [
        "In the scatter plot above, we can see three separate groups of data points and we would like to recover them using clustering -- think of \"discovering\" the class labels that we already take for granted in a classification task.\n",
        "\n",
        "Even if the groups are obvious in the data, it is hard to find them when the data lives in a high-dimensional space, which we can't visualize in a single histogram or scatterplot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FzCF9XwZG9qI"
      },
      "source": [
        "Now we will use one of the simplest clustering algorithms, K-means. This is an iterative algorithm which searches for three cluster centers such that the distance from each point to its cluster is minimized. The standard implementation of K-means uses the Euclidean distance, which is why we want to make sure that all our variables are measured on the same scale if we are working with real-world datastets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a-Fq7LmZG9qJ",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_6FgLXwKG9qO"
      },
      "source": [
        "We can get the cluster labels either by calling fit and then accessing the ``labels_`` attribute of the K means estimator, or by calling ``fit_predict``. Either way, the result contains the ID of the cluster that each point is assigned to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZRoVUc9ZG9qP",
        "colab": {}
      },
      "source": [
        "labels = kmeans.fit_predict(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DrtYVFF-G9qS",
        "colab": {}
      },
      "source": [
        "labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t3U1PqkWG9qY",
        "colab": {}
      },
      "source": [
        "np.all(y == labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iZLVLzqkG9qf"
      },
      "source": [
        "Let's visualize the assignments that have been found"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gEZ8GqDVG9qh",
        "colab": {}
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=labels);\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c=np.unique(labels), s=200, alpha=0.5);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8HjbwMMpG9qj"
      },
      "source": [
        "Compared to the true labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U61PKazyG9ql",
        "colab": {}
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DQ877MCgG9qq"
      },
      "source": [
        "Here, we are probably satisfied with the clustering results. But in general we might want to have a more quantitative evaluation. How about comparing our cluster labels with the ground truth we got when generating the blobs?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1yASpAmPG9qt",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "print('Accuracy score:', accuracy_score(y, labels))\n",
        "print(confusion_matrix(y, labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tzg-zMXeG9qw",
        "colab": {}
      },
      "source": [
        "np.mean(y == labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6WllOH9UG9qz"
      },
      "source": [
        "___\n",
        "## Exercise\n",
        "After looking at the \"True\" label array y, and the scatterplot and `labels` above, can you figure out why our computed accuracy is 0.0, not 1.0?\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3tuYY1BQG9q0"
      },
      "source": [
        "Even though we recovered the partitioning of the data into clusters perfectly, the cluster IDs we assigned were arbitrary, and we can not hope to recover them. Therefore, we must use a different scoring metric, such as ``adjusted_rand_score``, which is invariant to permutations of the labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DQFeKaK-G9q5",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "adjusted_rand_score(y, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RNub0p4FG9q-"
      },
      "source": [
        "One of the \"short-comings\" of K-means is that we have to specify the number of clusters, which we often don't know *apriori*. For example, let's have a look what happens if we set the number of clusters to 2 in our synthetic 3-blob dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r0N_GnNWG9q_",
        "colab": {}
      },
      "source": [
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels);\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='purple', s=200, alpha=0.5);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RunpCjmUG9rD",
        "colab": {}
      },
      "source": [
        "kmeans.cluster_centers_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jf1RePduG9rV"
      },
      "source": [
        "## Assumptions in Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Oo7RJlHUG9rW"
      },
      "source": [
        "**Clustering comes with assumptions**: A clustering algorithm finds clusters by making assumptions with samples should be grouped together. Each algorithm makes different assumptions and the quality and interpretability of your results will depend on whether the assumptions are satisfied for your goal. For K-means clustering, the model is that all clusters have equal, spherical variance.\n",
        "\n",
        "**In general, there is no guarantee that structure found by a clustering algorithm has anything to do with what you were interested in**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OZXwrhWdG9rY"
      },
      "source": [
        "We can easily create a dataset that has non-isotropic clusters, on which kmeans will fail:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SJByNctRG9rZ",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "n_samples = 1500\n",
        "random_state = 150\n",
        "X, y = make_blobs(n_samples=n_samples, random_state=random_state)\n",
        "\n",
        "# Incorrect number of clusters\n",
        "y_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)\n",
        "\n",
        "plt.subplot(221)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
        "plt.title(\"Incorrect Number of Blobs\")\n",
        "\n",
        "# Anisotropicly distributed data\n",
        "transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
        "X_aniso = np.dot(X, transformation)\n",
        "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_aniso)\n",
        "\n",
        "plt.subplot(222)\n",
        "plt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)\n",
        "plt.title(\"Anisotropicly Distributed Blobs\")\n",
        "\n",
        "# Different variance\n",
        "X_varied, y_varied = make_blobs(n_samples=n_samples,\n",
        "                                cluster_std=[1.0, 2.5, 0.5],\n",
        "                                random_state=random_state)\n",
        "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)\n",
        "\n",
        "plt.subplot(223)\n",
        "plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\n",
        "plt.title(\"Unequal Variance\")\n",
        "\n",
        "# Unevenly sized blobs\n",
        "X_filtered = np.vstack((X[y == 0][:400], X[y == 1][:200], X[y == 2][:10]))\n",
        "y_pred = KMeans(n_clusters=3,\n",
        "                random_state=random_state).fit_predict(X_filtered)\n",
        "\n",
        "plt.subplot(224)\n",
        "plt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)\n",
        "plt.title(\"Unevenly Sized Blobs\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u8JGwvlKG9ri"
      },
      "source": [
        "___\n",
        "## Exercise\n",
        "* Perform K-means clustering on the digits data, searching for ten clusters.\n",
        "* Visualize the cluster centers as images (i.e. reshape each to 8x8 and use ``plt.imshow``) with the code provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HaDlPxCgG9rj",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# load data\n",
        "digits = load_digits()\n",
        "\n",
        "# perform clustering, the data is available in digits.data\n",
        "\n",
        "# kmeans = ...\n",
        "# ...\n",
        "\n",
        "# visualize the cluster centers\n",
        "fig = plt.figure(figsize=(8, 3))\n",
        "for i in range(10):\n",
        "    ax = fig.add_subplot(2, 5, 1 + i)\n",
        "    ax.imshow(kmeans.cluster_centers_[i].reshape((8, 8)),\n",
        "              cmap=plt.cm.binary)\n",
        "    ax.axis(\"off\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PJhU1rZ6G9rp"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE0HgIdKNGez",
        "colab_type": "text"
      },
      "source": [
        "## Dimensionality reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9WurOzpNT09",
        "colab_type": "text"
      },
      "source": [
        "Remeber the two different algorithms to do dimensionality reduction:\n",
        "* PCA - linear method\n",
        "* t-SNE - non-linear method (can be slow if data is large)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFcKqIvkNpcS",
        "colab_type": "text"
      },
      "source": [
        "Follow the link to play around with the visualizations: <a href=\"https://projector.tensorflow.org/ \" target=\"_blank\">https://projector.tensorflow.org/ </a>\n",
        "\n",
        "\n",
        "<img width=\"60%\" src='https://github.com/fordanic/cmiv-ai-course/blob/master/notebooks/figures/tsne_mnist.PNG?raw=1'/>\n",
        "\n",
        "What to try yourself? Code available in appendix below on both PCA and t-SNE!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IrQroRWNCrF",
        "colab_type": "text"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_jl2xM-tG9rP"
      },
      "source": [
        "## The Elbow Method \n",
        "\n",
        "The Elbow method is a \"rule-of-thumb\" approach to finding the optimal number of clusters for k-means clustering. Here, we look at the cluster dispersion for different values of k:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dawgL6NDG9rR",
        "colab": {}
      },
      "source": [
        "# Create new data set with five clusters\n",
        "X, y = make_blobs(n_samples=1000, centers=5, random_state=42)\n",
        "plt.figure()\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y);\n",
        "\n",
        "# Perform K-means clustering and measure the distortion to assess the fit of the clustering\n",
        "distortions = []\n",
        "for i in range(1, 15):\n",
        "    km = KMeans(n_clusters=i, \n",
        "                random_state=0)\n",
        "    km.fit(X)\n",
        "    distortions.append(km.inertia_)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(1, 15), distortions, marker='o')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Distortion')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MnZZfawQG9rU"
      },
      "source": [
        "Then, we pick the value that resembles the \"pit of an elbow.\" As we can see, this would be k=3 in this case, which makes sense given our visual expection of the dataset previously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0CXA3ovW5cL",
        "colab_type": "text"
      },
      "source": [
        "___\n",
        "## Exercise\n",
        "Explore the elbow method while using the make_blob function with different number of centers, for example try values between 3 and 10. Is it easy to spot the true elbow? Why or why not? \n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JDnaFuHhG9rr"
      },
      "source": [
        "## Dimensionality Reduction on Breast Cancer Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "htCCPXryG9ru"
      },
      "source": [
        "### PCA\n",
        "We start by loading the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lA2snoDRG9rx",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Get the data of interest\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XpvnRhCuG9rz"
      },
      "source": [
        "Remember that each samples consists of 30 features, i.e. the data is 30 dimensional. However, using PCA we will reduce the number of dimensions to two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lGar6LTNG9r3",
        "colab": {}
      },
      "source": [
        "from sklearn import decomposition\n",
        "\n",
        "pca = decomposition.PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "X_reduced = pca.transform(X)\n",
        "print(X_reduced.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mawqjwFpG9sB"
      },
      "source": [
        "Now will plot the projected data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RZHF55hAG9sB",
        "colab": {}
      },
      "source": [
        "# Initiate a plot\n",
        "fig, ax = plt.subplots(1,1,figsize=(8, 8))\n",
        "\n",
        "# Plot labels\n",
        "for name, label in [('Benign', 0), ('Malignant', 1)]:\n",
        "    plt.text(X_reduced[y == label, 0].mean(),\n",
        "             X_reduced[y == label, 1].mean(), name,\n",
        "             bbox=dict(alpha=1.0, edgecolor='w', facecolor='w'))\n",
        "\n",
        "# Plot projected data\n",
        "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=plt.cm.Spectral, edgecolor='k')\n",
        "ax.set_xticklabels([])\n",
        "ax.set_yticklabels([])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wcUH0M6pG9sG"
      },
      "source": [
        "### t-SNE\n",
        "Let's do the same thing but using t-SNE. However, the computations this time will take a little bit longer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SPN5zUyOG9sH",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Get the data of interest\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "X = TSNE(n_components=2).fit_transform(X)\n",
        "\n",
        "# Initiate a plot\n",
        "fig, ax = plt.subplots(1,1,figsize=(8, 8))\n",
        "\n",
        "# Plot labels\n",
        "for name, label in [('Benign', 0), ('Malignant', 1)]:\n",
        "    plt.text(X[y == label, 0].mean(),\n",
        "             X[y == label, 1].mean(), name,\n",
        "             bbox=dict(alpha=1.0, edgecolor='w', facecolor='w'))\n",
        "\n",
        "# Plot projected data\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral, edgecolor='k')\n",
        "ax.set_xticklabels([])\n",
        "ax.set_yticklabels([])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x4c-5xJHG9sO"
      },
      "source": [
        "## Dimensionality Reduction on Digits Dataset (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3K_PBeHYG9sQ"
      },
      "source": [
        "### PCA\n",
        "We start by loading the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "39H47cCEG9sR",
        "colab": {}
      },
      "source": [
        "from sklearn import decomposition\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "# Get the data of interest\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l6BsqH3tG9sT"
      },
      "source": [
        "Remember that each digit consists of 8x8 pixels/features, i.e. the data is 64 dimensional. However, using PCA we will reduce the number of dimensions to two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JtYK_d8aG9sV",
        "colab": {}
      },
      "source": [
        "pca = decomposition.PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "X = pca.transform(X)\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjUgDzQBYfGY",
        "colab_type": "text"
      },
      "source": [
        "___\n",
        "### Exercise\n",
        "Before plotting our 8x8 data recuded to two dimensions, briefly consider which numbers you expect seeing close to each other, i.e. which number have similar shapes?\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IC7ItSBqG9sX"
      },
      "source": [
        "Now will plot the projected data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k3QVzZspG9sY",
        "colab": {}
      },
      "source": [
        "# Initiate a plot\n",
        "fig, ax = plt.subplots(1,1,figsize=(8, 8))\n",
        "\n",
        "# Plot labels\n",
        "for name, label in [('0', 0), ('1', 1), ('2', 2), ('3', 3), ('4', 4), ('5', 5), ('6', 6), ('7', 7), ('8', 8), ('9', 9)]:\n",
        "    plt.text(X[y == label, 0].mean(),\n",
        "             X[y == label, 1].mean(), name,\n",
        "             bbox=dict(alpha=1.0, edgecolor='w', facecolor='w'))\n",
        "\n",
        "# Plot projected data\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral, edgecolor='k')\n",
        "ax.set_xticklabels([])\n",
        "ax.set_yticklabels([])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvZ17sXJY2s6",
        "colab_type": "text"
      },
      "source": [
        "___\n",
        "### Exercise\n",
        "Did the plot and the distribution of the different numbers resemble your earlier guesses?\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gEYlfvKSG9sa"
      },
      "source": [
        "### t-SNE\n",
        "Let's do the same thing but using t-SNE. However, the computations this time will take a little bit longer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GeVg8-URG9sb",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "# Get the data of interest\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "X = TSNE(n_components=2).fit_transform(X)\n",
        "\n",
        "# Initiate a plot\n",
        "fig, ax = plt.subplots(1,1,figsize=(8, 8))\n",
        "\n",
        "# Plot labels\n",
        "for name, label in [('0', 0), ('1', 1), ('2', 2), ('3', 3), ('4', 4), ('5', 5), ('6', 6), ('7', 7), ('8', 8), ('9', 9)]:\n",
        "    plt.text(X[y == label, 0].mean(),\n",
        "             X[y == label, 1].mean(), name,\n",
        "             bbox=dict(alpha=1.0, edgecolor='w', facecolor='w'))\n",
        "\n",
        "# Plot projected data\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral, edgecolor='k')\n",
        "ax.set_xticklabels([])\n",
        "ax.set_yticklabels([])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NdHrsqMhG9sh"
      },
      "source": [
        "___\n",
        "## Exercises (optional)\n",
        "Perform K-means clustering on the projected digits data, i.e. the data reduced to 2 dimensions. Eiter using PCA or t-SNE.\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCjka5Qa3ObD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}