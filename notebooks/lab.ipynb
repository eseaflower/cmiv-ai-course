{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a9837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio semantic-kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dfb5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!wget -O assets.tar https://github.com/eseaflower/cmiv-ai-course/raw/master/notebooks/assets.tar\n",
    "!tar -xvf assets.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b16e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"API_KEY\": \"\",\n",
    "    \"DEPLOYMENT\": \"\",\n",
    "    \"ENDPOINT\": \"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c081f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Agent code\n",
    "\n",
    "import copy\n",
    "import inspect\n",
    "from functools import wraps\n",
    "from typing import Annotated, Any, AsyncGenerator, Literal, Mapping, Optional\n",
    "import PIL\n",
    "import PIL.Image\n",
    "from pydantic import Field, model_validator\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureChatPromptExecutionSettings\n",
    "from semantic_kernel.kernel import Kernel, ChatHistory\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.functions.kernel_function_decorator import kernel_function\n",
    "from semantic_kernel.functions import KernelPlugin\n",
    "from semantic_kernel.contents import ImageContent\n",
    "import io\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "\n",
    "# Create a shallow chat service\n",
    "_shallow_service = AzureChatCompletion(service_id=\"agent\",\n",
    "                                       api_key=settings[\"API_KEY\"],\n",
    "                                       deployment_name=settings[\"DEPLOYMENT\"],\n",
    "                                       endpoint=settings[\"ENDPOINT\"],\n",
    "                                       api_version=\"2024-02-15-preview\")\n",
    "\n",
    "\n",
    "class AgentType(Enum):\n",
    "    Shallow = \"shallow\"\n",
    "\n",
    "\n",
    "def get_service(agent_type: AgentType):\n",
    "    if agent_type == AgentType.Shallow:\n",
    "        return _shallow_service\n",
    "    else:\n",
    "        raise ValueError(\"Invalid agent type: \", agent_type)\n",
    "\n",
    "\n",
    "class AgentExecutionSettings(AzureChatPromptExecutionSettings):\n",
    "    \"\"\"Class that enables switching between different agent types.\"\"\"\n",
    "    agent_type: Annotated[AgentType | None, Field(exclude=True)] = None\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    def validate_service_settings(cls, model: Any) -> Any:\n",
    "        \"\"\"Update some execution settings that differ based on agent type.\"\"\"\n",
    "        if isinstance(model, dict):\n",
    "            model_type = model.get(\"agent_type\")\n",
    "        return model\n",
    "\n",
    "\n",
    "class InvocationRecord:\n",
    "    def __init__(self, instance_id: str, invocation: Any):\n",
    "        self.instance_id = instance_id\n",
    "        self.invocation = invocation\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.instance_id}: {self.invocation}\"\n",
    "\n",
    "\n",
    "class InvocationLog:\n",
    "    def __init__(self):\n",
    "        self.invocations = []\n",
    "\n",
    "    def add_invocation(self, invocation: InvocationRecord):\n",
    "        self.invocations.append(invocation)\n",
    "\n",
    "    def reset(self):\n",
    "        self.invocations = []\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\n\".join([str(inv) for inv in self.invocations])\n",
    "\n",
    "\n",
    "class InvocationLogger:\n",
    "    def __init__(self, instance_id: Optional[str] = None):\n",
    "        self.log: Optional[InvocationLog] = None\n",
    "        self.instance_id = instance_id or \"_\"\n",
    "\n",
    "    def set_log(self, log: InvocationLog):\n",
    "        self.log = log\n",
    "\n",
    "    def create_record(self, invocation) -> InvocationRecord:\n",
    "        return InvocationRecord(self.instance_id, invocation)\n",
    "\n",
    "    def log_invocation(self, invocation):\n",
    "        if self.log:\n",
    "            self.log.add_invocation(self.create_record(invocation))\n",
    "\n",
    "\n",
    "class BaseAgent(InvocationLogger):\n",
    "    def __init__(self, agent_type: AgentType, instance_id: Optional[str] = None):\n",
    "        super().__init__(instance_id=instance_id)\n",
    "\n",
    "        # Create the kernel\n",
    "        self.kernel = Kernel()\n",
    "        self.agent_type = agent_type\n",
    "        self.service = get_service(self.agent_type)\n",
    "        # Must register the service with the kernel\n",
    "        self.kernel.add_service(self.service)\n",
    "        self.logger_plugins: list[InvocationLogger] = []\n",
    "\n",
    "    def create_history(self, instructions: str | None = None) -> ChatHistory:\n",
    "        chat = ChatHistory()\n",
    "        if instructions:\n",
    "            chat.add_system_message(instructions)\n",
    "        return chat\n",
    "\n",
    "\n",
    "# Maybe have a DynamicPlugin that is invoked with the KernelPlugin that\n",
    "# was registered with the kernel. The DynamicPlugin could override function\n",
    "# descriptions for instance. The description can be dug out from the\n",
    "# KernelPlugin[fn_name]->KernelFunction.metadatas.description\n",
    "\n",
    "    def add_plugin(self, plugin, plugin_name: str, description: Optional[str] = None) -> KernelPlugin:\n",
    "        if isinstance(plugin, InvocationLogger):\n",
    "            self.logger_plugins.append(plugin)\n",
    "        return self.kernel.add_plugin(plugin, plugin_name, description=description)\n",
    "\n",
    "    def set_log(self, log: InvocationLog):\n",
    "        super().set_log(log)\n",
    "        for plugin in self.logger_plugins:\n",
    "            plugin.set_log(log)\n",
    "\n",
    "\n",
    "class ChatAgent(BaseAgent):\n",
    "    def __init__(self,\n",
    "                 agent_type: AgentType = AgentType.Shallow,\n",
    "                 max_tokens: Optional[int] = None,\n",
    "                 temperature: Optional[float] = None,\n",
    "                 reasoning_effort: Literal[\"low\",\n",
    "                                           \"medium\", \"high\"] | None = None,\n",
    "                 instance_id: Optional[str] = None):\n",
    "\n",
    "        super().__init__(agent_type, instance_id=instance_id)\n",
    "\n",
    "        # Use the custom settings class to support different agent types\n",
    "        self.chat_settings = AgentExecutionSettings(\n",
    "            agent_type=agent_type,  # Need to set the agent type\n",
    "            service_id=self.service.service_id,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            reasoning_effort=reasoning_effort,\n",
    "            function_choice_behavior=FunctionChoiceBehavior.Auto(),\n",
    "        )\n",
    "\n",
    "    def _get_settings(self, disable_function_calls: bool = False) -> AgentExecutionSettings:\n",
    "        # Use the default settings unless we disable function calls\n",
    "        settings = self.chat_settings\n",
    "        if disable_function_calls:\n",
    "            # Create a copy of the settings since the agent\n",
    "            # could be used by multiple users with different behavior.\n",
    "            settings = copy.deepcopy(self.chat_settings)\n",
    "            settings.function_choice_behavior = FunctionChoiceBehavior.NoneInvoke(\n",
    "                filters={\"included_plugins\": [], \"included_functions\": []})\n",
    "        return settings\n",
    "\n",
    "    async def chat(self, history: ChatHistory, disable_function_calls: bool = False) -> str | None:\n",
    "        # Get settings for this invocation\n",
    "        settings = self._get_settings(\n",
    "            disable_function_calls=disable_function_calls)\n",
    "\n",
    "        # Invoke the chat function\n",
    "        response = await self.service.get_chat_message_content(history, settings, kernel=self.kernel)\n",
    "        # Return the response\n",
    "        return str(response)\n",
    "\n",
    "    async def chat_streaming(self, history: ChatHistory, disable_function_calls: bool = False) -> AsyncGenerator[str, Any]:\n",
    "        # Get settings for this invocation\n",
    "        settings = self._get_settings(\n",
    "            disable_function_calls=disable_function_calls)\n",
    "        # Invoke the chat function and stream the response\n",
    "        async for response in self.service.get_streaming_chat_message_content(history, settings, kernel=self.kernel):\n",
    "            yield str(response)\n",
    "\n",
    "\n",
    "# Decorator to log function enter and exit\n",
    "def log_enter_exit(func):\n",
    "\n",
    "    if inspect.iscoroutinefunction(func):\n",
    "        @wraps(func)\n",
    "        async def async_wrapper(*args, **kwargs):\n",
    "            _self = args[0]\n",
    "            fn_context = f\"{_self.__class__.__name__}::{func.__name__}\"\n",
    "            _self.log_invocation(f\"Entering(A): {fn_context}\")\n",
    "            result = await func(*args, **kwargs)\n",
    "            _self.log_invocation(f\"Exiting(A): {fn_context}\")\n",
    "            return result\n",
    "\n",
    "        return async_wrapper\n",
    "    else:\n",
    "        @wraps(func)\n",
    "        def sync_wrapper(*args, **kwargs):\n",
    "            _self = args[0]\n",
    "            fn_context = f\"{_self.__class__.__name__}::{func.__name__}\"\n",
    "            _self.log_invocation(f\"Entering: {fn_context}\")\n",
    "            result = func(*args, **kwargs)\n",
    "            _self.log_invocation(f\"Exiting: {fn_context}\")\n",
    "            return result\n",
    "        return sync_wrapper\n",
    "\n",
    "\n",
    "class PythonCodePlugin(InvocationLogger):\n",
    "    def __init__(self, instance_id: Optional[str] = None):\n",
    "        super().__init__(instance_id=instance_id)\n",
    "\n",
    "    @kernel_function(name=\"run_python_script\", description=\"Run a Python script. If a value should be returned, assign it to the variabel 'result'\")\n",
    "    @log_enter_exit\n",
    "    def run_python_script(self, code: Annotated[str, \"The Python script to run. The code must return any result by assigning the variable 'result'\"]) -> Annotated[Optional[object], \"The output of the code\"]:\n",
    "        self.log_invocation(f\"Running code:\\n {code}\")\n",
    "\n",
    "        # context = {'column': 'result'}\n",
    "\n",
    "        exec_locals: Mapping[str, object] = {}\n",
    "        exec_globals = None  # {'context': context}\n",
    "        try:\n",
    "            exec(code, exec_globals, exec_locals)\n",
    "            result = exec_locals.get('result', None)\n",
    "            self.log_invocation(f\"Result of code execution: {result}\")\n",
    "            return result if result is not None else \"Success, no result returned\"\n",
    "        except Exception as e:\n",
    "            self.log_invocation(f\"Error running code: {e}\")\n",
    "            return str(e)\n",
    "\n",
    "\n",
    "class PythonCoderAgent(ChatAgent):\n",
    "\n",
    "    _instructions: str = \"\"\"\n",
    "    You are a python coder agent that solves problems by generating Python code and executing it. \n",
    "    To execute the python code you have acces to tools that can run Python code.\"\"\"\n",
    "\n",
    "    def __init__(self, instance_id: Optional[str] = None):\n",
    "        super().__init__(instance_id=instance_id)\n",
    "        self.add_plugin(PythonCodePlugin(), \"python_code_plugin\")\n",
    "\n",
    "    @kernel_function(name=\"code\", description=\"Solves tasks by generating Python code for the task and executing it.\")\n",
    "    @log_enter_exit\n",
    "    async def code(self, task: Annotated[str, \"A description of the task that should be solved by coding.\"]) -> Annotated[Optional[str], \"A solution of the task obtained by coding.\"]:\n",
    "        self.log_invocation(f\"Task:\\n {task}\")\n",
    "        history = self.create_history(\n",
    "            instructions=PythonCoderAgent._instructions)\n",
    "        history.add_user_message(\n",
    "            f\"Please help me solve the following task: {task}\")\n",
    "        response = await self.chat(history)\n",
    "        self.log_invocation(response)\n",
    "        return response\n",
    "\n",
    "\n",
    "def to_image_content(image: PIL.Image.Image, format: str = \"JPEG\") -> ImageContent:\n",
    "\n",
    "    if format not in [\"JPEG\", \"PNG\"]:\n",
    "        raise ValueError(\n",
    "            f\"Invalid image format. Supported formats are 'JPEG' and 'PNG'. Got: {format}\")\n",
    "\n",
    "    mime_type = \"image/jpeg\"\n",
    "    if format == \"PNG\":\n",
    "        mime_type = \"image/png\"\n",
    "\n",
    "    if format == \"JPEG\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    image_buffer = io.BytesIO()\n",
    "    image.save(image_buffer, format=format)\n",
    "\n",
    "    image_buffer.seek(0)\n",
    "    image_bytes = image_buffer.read()\n",
    "    return ImageContent(\n",
    "        data=image_bytes, data_format=\"base64\", mime_type=mime_type)\n",
    "\n",
    "\n",
    "class ImageQueryAgent(ChatAgent):\n",
    "    _instructions: str = \"\"\"\n",
    "    You are an image query agent that answers questions about images. Your answers should be based on the content of the image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, instance_id: Optional[str] = None):\n",
    "        super().__init__(AgentType.Shallow, instance_id=instance_id)\n",
    "\n",
    "    \n",
    "    def create_history(self, instructions=None):\n",
    "        instructions = instructions or ImageQueryAgent._instructions\n",
    "        return super().create_history(instructions=instructions)\n",
    "\n",
    "    async def query(self, task: str, image: PIL.Image.Image) -> Optional[str]:\n",
    "        history = self.create_history()\n",
    "        image_content = to_image_content(image)\n",
    "        history.add_user_message([image_content])\n",
    "        history.add_user_message(task)\n",
    "        response = await self.chat(history)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c71ad85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Interaction code\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, AsyncGenerator, Optional, Tuple, List\n",
    "import PIL\n",
    "import PIL.Image\n",
    "from gradio import ChatMessage, Image as GrImage, Audio\n",
    "from semantic_kernel.contents import AuthorRole\n",
    "\n",
    "\n",
    "def _history_to_gradio(history: ChatHistory) -> list[ChatMessage]:\n",
    "    \"\"\"\n",
    "    Convert semantic kernel chat history to a list of ChatMessage objects.\n",
    "    Returns:\n",
    "        list: List of ChatMessage objects\n",
    "    \"\"\"\n",
    "    gradio_history = []\n",
    "    for message in history:\n",
    "        if message.role == AuthorRole.USER:\n",
    "            gradio_history.append(ChatMessage(\n",
    "                role=\"user\", content=message.content))\n",
    "        elif message.role == AuthorRole.ASSISTANT:\n",
    "            gradio_history.append(ChatMessage(\n",
    "                role=\"assistant\", content=message.content))\n",
    "    return gradio_history\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UserMessage:\n",
    "    message: Optional[str]\n",
    "    image: Optional[PIL.Image.Image]\n",
    "    files: Optional[List[str]]\n",
    "\n",
    "\n",
    "def get_gradio_messages(message: UserMessage) -> list[ChatMessage]:\n",
    "    output = []\n",
    "    for f in message.files:\n",
    "        if f.endswith(\".jpg\") or f.endswith(\".png\"):\n",
    "            print(f)\n",
    "            output.append(ChatMessage(role=\"user\", content=GrImage(f)))\n",
    "        if f.endswith(\".wav\") or f.endswith(\".mp3\"):\n",
    "            output.append(ChatMessage(role=\"user\", content=Audio(f)))\n",
    "    if message.message is not None:\n",
    "        output.append(ChatMessage(role=\"user\", content=message.message))\n",
    "    return output\n",
    "\n",
    "\n",
    "_custom_instructions: Optional[str] = None\n",
    "\n",
    "class InteractionAgent:\n",
    "    \"\"\"\n",
    "    A class to handle the interaction between the user and the chat agent.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.agent = ChatAgent(instance_id=\"interaction_agent\")\n",
    "\n",
    "        self.history = self.agent.create_history(_custom_instructions)\n",
    "        self.gradio_history = []\n",
    "\n",
    "    async def respond_streaming(self,\n",
    "                                message: UserMessage,\n",
    "                                call_log: Optional[InvocationLog] = None) -> AsyncGenerator[Tuple[list[ChatMessage]], Any]:\n",
    "        \"\"\"\n",
    "        Process the user message and return a stream of updated chat history.\n",
    "        Args:\n",
    "            message: The user's input message\n",
    "        Returns:\n",
    "            AsyncGenerator[list[ChatMessage], Any] - Stream of updated history of the conversation\n",
    "        \"\"\"\n",
    "\n",
    "        # Log the call\n",
    "        call_log = call_log or InvocationLog()\n",
    "        self.agent.set_log(call_log)\n",
    "\n",
    "        # Add current message\n",
    "        self._add_user_message(message)\n",
    "\n",
    "        # Add the user message to the gradio history\n",
    "        self.gradio_history.extend(get_gradio_messages(message))\n",
    "\n",
    "        yield self.gradio_history\n",
    "\n",
    "        current_response_message = ChatMessage(role=\"assistant\", content=\"\")\n",
    "        self.gradio_history.append(current_response_message)\n",
    "\n",
    "        current_response: str = \"\"\n",
    "        async for response_chunk in self.agent.chat_streaming(self.history):\n",
    "            # Update the response with the new chunk\n",
    "            current_response += response_chunk\n",
    "            # Update the content in-place\n",
    "            current_response_message.content = current_response\n",
    "            # Yield the updated history\n",
    "            yield self.gradio_history\n",
    "\n",
    "\n",
    "        # Add the final response to the history\n",
    "        self.history.add_assistant_message(current_response)\n",
    "\n",
    "        print(f\"Invocations:\\n {call_log}\")\n",
    "\n",
    "    def reset(self) -> list[ChatMessage]:\n",
    "        \"\"\"\n",
    "        Reset the chat history.\n",
    "        \"\"\"\n",
    "        self.history.clear()\n",
    "        self.gradio_history = []\n",
    "        return self.gradio_history\n",
    "\n",
    "    def _add_user_message(self, message: UserMessage):\n",
    "        \"\"\"\n",
    "        Add a user message to the chat history.\n",
    "        \"\"\"\n",
    "        if message.image is not None:\n",
    "            image_content = to_image_content(message.image)\n",
    "            self.history.add_user_message([image_content])\n",
    "\n",
    "        if message.message is not None:\n",
    "            self.history.add_user_message(message.message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52008a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Gradio code\n",
    "from typing import Any, AsyncGenerator, Optional, Tuple\n",
    "import gradio as gr\n",
    "import PIL.Image\n",
    "from PIL.Image import Image\n",
    "\n",
    "\n",
    "async def respond_streaming(agent: InteractionAgent,\n",
    "                            msg: dict[str, Any],\n",
    "                            ) -> AsyncGenerator[Tuple[str, list[gr.ChatMessage], str], Any]:\n",
    "    \"\"\"\n",
    "    Respond to the user's message and return the updated chat history.\n",
    "    Args:\n",
    "        agent: The interaction agent\n",
    "        msg: The user's input message\n",
    "    Returns:\n",
    "        AsyncGenerator[(\"\", None, list[ChatMessage])]: Async generator with an empty string and updated history of the conversation\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    upload_image = None\n",
    "    text = msg[\"text\"]\n",
    "\n",
    "    for input_file in msg[\"files\"]:\n",
    "        if input_file.endswith(\".jpg\") or input_file.endswith(\".png\"):\n",
    "            upload_image = PIL.Image.open(input_file).convert(\"RGB\")\n",
    "\n",
    "    user_message = UserMessage(\n",
    "        message=text, image=upload_image, files=msg[\"files\"])\n",
    "\n",
    "    # Create an invocation log to keep track of the calls.\n",
    "    call_log = InvocationLog()\n",
    "    \n",
    "    async for history in agent.respond_streaming(user_message, call_log=call_log):\n",
    "        yield \"\", history\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def clear_history(agent: InteractionAgent) -> Tuple[str, list[gr.ChatMessage], str]:\n",
    "    \"\"\"\n",
    "    Clear the chat history.\n",
    "    Args:\n",
    "        agent: The interaction agent\n",
    "    Returns:\n",
    "        (\"\", None, list[ChatMessage]): Empty string and an empty list\n",
    "\n",
    "    \"\"\"\n",
    "    return \"\", agent.reset()\n",
    "\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks(theme=gr.themes.Base(), fill_height=True) as demo:\n",
    "\n",
    "    # The InteractionAgent is not deep-copiable. We can get around this\n",
    "    # by initializing it in a lambda function.\n",
    "    interaction_agent = gr.State(lambda: InteractionAgent())\n",
    "\n",
    "    # Add a title\n",
    "    # gr.Markdown(\"# Simple Chat Demo\")\n",
    "\n",
    "    # Create the chatbot component\n",
    "    chatbot = gr.Chatbot(\n",
    "        scale=1,        \n",
    "        height=600,\n",
    "        show_label=False,\n",
    "        type=\"messages\",\n",
    "        layout=\"panel\",\n",
    "        avatar_images=('assets/radiologist_user_avatar_small.png',\n",
    "                       'assets/assistant_avatar_small.png'),\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        msg = gr.MultimodalTextbox(\n",
    "            interactive=True,\n",
    "            file_count=\"multiple\",\n",
    "            placeholder=\"Enter message or upload file...\",\n",
    "            show_label=False,\n",
    "            sources=[\"upload\"],\n",
    "            scale=9\n",
    "        )\n",
    "        clear_btn = gr.Button(\"Clear Chat\", scale=1)\n",
    "\n",
    "           \n",
    "    # Set up event handlers\n",
    "    gr.on(\n",
    "        triggers=[msg.submit],\n",
    "        fn=respond_streaming,\n",
    "        inputs=[interaction_agent, msg],\n",
    "        outputs=[msg, chatbot])\n",
    "\n",
    "    gr.on(triggers=[clear_btn.click, chatbot.clear],\n",
    "          fn=clear_history,\n",
    "          inputs=[interaction_agent],\n",
    "          outputs=[msg, chatbot],\n",
    "          show_progress=\"hidden\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6909b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_demo():\n",
    "    if demo:\n",
    "        demo.close()\n",
    "\n",
    "    demo.launch(inline=True, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14a4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/gradio/6502ce0140c5e20df68d266e633b9aa42f697036d01133bbdbb3c217b700fd83/radiologist_user_avatar_small.png\n",
      "Invocations:\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Set the custom instructions for the model to use\n",
    "_custom_instructions = \"\"\" \n",
    "You are an expert radiologist. You should always use your experties to answer the users questions.\n",
    "\"\"\"\n",
    "launch_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12454210",
   "metadata": {},
   "outputs": [],
   "source": [
    "CR_report = \"\"\" \n",
    "*Findings*\n",
    "Lungs:\n",
    "The lung fields appear hyperinflated bilaterally with increased bronchial markings noted.\n",
    "No acute consolidation or focal opacities are observed.\n",
    "Mild patchy opacities are seen in the right lower zone, suggesting possible early signs of atelectasis or minimal fluid.\n",
    "Cardiac Silhouette:\n",
    "The heart size is within normal limits.\n",
    "Cardiac contours are well-defined with no significant enlargement or signs of congestive heart failure.\n",
    "Mediastinum:\n",
    "The mediastinal contour appears within normal limits without evidence of mediastinal shift.\n",
    "Diaphragm:\n",
    "The diaphragms are situated normally, with no signs of elevation or flattening.\n",
    "Costophrenic Angles:\n",
    "Costophrenic angles are sharp. No free air or pleural effusion is observed.\n",
    "Bones and Soft Tissues:\n",
    "No acute bony abnormalities noted.\n",
    "Soft tissues appear unremarkable.\n",
    "\n",
    "*Impression*\n",
    "Findings consistent with mild hyperinflation, possibly suggesting early obstructive airway disease. \n",
    "No acute process identified. Recommend clinical correlation and follow-up if respiratory symptoms persist. \n",
    "Consider further evaluation with a CT scan if clinically indicated.\n",
    "\"\"\"\n",
    "\n",
    "CR_AI_findings = {\n",
    "    \"ChextXpert\": {\n",
    "        \"Nodules and Masses\": \"Small nodules in the right upper lobe, no masses.\",\n",
    "        \"Pleural Effusion\": \"Mild pleural effusion.\",\n",
    "    },\n",
    "    \"DeepMed\": {\n",
    "        \"Pneumonia\": \"No signs of pneumonia.\",\n",
    "        \"Pulmonary Edema\": \"Visible signs of pulmonary edema.\",\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf290afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "async def check_report(report_text, ai_findings):\n",
    "    findings_str = json.dumps(ai_findings, indent=4)\n",
    "\n",
    "    message = f\"\"\"\n",
    "    Help me check the following report against the findings from the AI models.\n",
    "    If there are any discrepancies, please point them out.\n",
    "    \n",
    "    =Report=\n",
    "    {report_text}\n",
    "\n",
    "    =AI Findings=\n",
    "    {findings_str}\"\"\"\n",
    "\n",
    "    check_agent = ChatAgent(instance_id=\"check_agent\")\n",
    "    history = check_agent.create_history()\n",
    "    history.add_user_message(message)\n",
    "    response = await check_agent.chat(history)\n",
    "    return response\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979b60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "check_response = await check_report(CR_report, CR_AI_findings)\n",
    "Markdown(check_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
